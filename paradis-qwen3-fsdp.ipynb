{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/tqhb2502/PARADIS-final_project.git","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-21T17:32:45.942544Z","iopub.execute_input":"2025-06-21T17:32:45.942745Z","iopub.status.idle":"2025-06-21T17:32:47.048537Z","shell.execute_reply.started":"2025-06-21T17:32:45.942727Z","shell.execute_reply":"2025-06-21T17:32:47.047723Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'PARADIS-final_project'...\nremote: Enumerating objects: 241, done.\u001b[K\nremote: Counting objects: 100% (88/88), done.\u001b[K\nremote: Compressing objects: 100% (41/41), done.\u001b[K\nremote: Total 241 (delta 31), reused 84 (delta 27), pack-reused 153 (from 1)\u001b[K\nReceiving objects: 100% (241/241), 51.10 KiB | 2.04 MiB/s, done.\nResolving deltas: 100% (81/81), done.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"%cd PARADIS-final_project","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T17:32:47.049705Z","iopub.execute_input":"2025-06-21T17:32:47.049979Z","iopub.status.idle":"2025-06-21T17:32:47.058443Z","shell.execute_reply.started":"2025-06-21T17:32:47.049939Z","shell.execute_reply":"2025-06-21T17:32:47.057789Z"}},"outputs":[{"name":"stdout","text":"/kaggle/working/PARADIS-final_project\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!git pull","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T18:20:10.725561Z","iopub.execute_input":"2025-06-21T18:20:10.726153Z","iopub.status.idle":"2025-06-21T18:20:11.137960Z","shell.execute_reply.started":"2025-06-21T18:20:10.726122Z","shell.execute_reply":"2025-06-21T18:20:11.137114Z"}},"outputs":[{"name":"stdout","text":"remote: Enumerating objects: 5, done.\u001b[K\nremote: Counting objects: 100% (5/5), done.\u001b[K\nremote: Compressing objects: 100% (2/2), done.\u001b[K\nremote: Total 3 (delta 1), reused 3 (delta 1), pack-reused 0 (from 0)\u001b[K\nUnpacking objects: 100% (3/3), 328 bytes | 328.00 KiB/s, done.\nFrom https://github.com/tqhb2502/PARADIS-final_project\n   4fde200..8e7a445  master     -> origin/master\nUpdating 4fde200..8e7a445\nFast-forward\n PARADIS-Qwen3-FSDP.py | 2 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n 1 file changed, 1 insertion(+), 1 deletion(-)\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"!torchrun --nproc_per_node=2 PARADIS-Qwen3-FSDP.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-21T18:20:13.009815Z","iopub.execute_input":"2025-06-21T18:20:13.010406Z","iopub.status.idle":"2025-06-21T18:21:06.817025Z","shell.execute_reply.started":"2025-06-21T18:20:13.010352Z","shell.execute_reply":"2025-06-21T18:21:06.816328Z"}},"outputs":[{"name":"stdout","text":"W0621 18:20:14.856000 1044 torch/distributed/run.py:792] \nW0621 18:20:14.856000 1044 torch/distributed/run.py:792] *****************************************\nW0621 18:20:14.856000 1044 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \nW0621 18:20:14.856000 1044 torch/distributed/run.py:792] *****************************************\n2025-06-21 18:20:20.835631: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-06-21 18:20:20.835715: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1750530020.858262    1047 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1750530020.858260    1046 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1750530020.865455    1047 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nE0000 00:00:1750530020.865711    1046 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n[Rank 0] Starting with world_size = 2\n[Rank 1] Starting with world_size = 2\n[Rank 0] Process group initialized!\n[Rank 0] Backend: nccl\n[Rank 0] World Size: 2\n[Rank 0] NCCL_DEBUG: NOT SET\n[Rank 1] Process group initialized!\n[Rank 1] Backend: nccl\n[Rank 1] World Size: 2\n[Rank 1] NCCL_DEBUG: NOT SET\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtqhb2502\u001b[0m (\u001b[33mtqhb2502-hanoi-university-of-science-and-technology\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `shared` mode feature is experimental and may change. Please contact support@wandb.com for guidance and to report any issues.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtqhb2502\u001b[0m (\u001b[33mtqhb2502-hanoi-university-of-science-and-technology\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/PARADIS-final_project/wandb/run-20250621_182025-f6jv04dx\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mFSDP\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/tqhb2502-hanoi-university-of-science-and-technology/PARADIS-Qwen3_1.7B\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/tqhb2502-hanoi-university-of-science-and-technology/PARADIS-Qwen3_1.7B/runs/f6jv04dx\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `shared` mode feature is experimental and may change. Please contact support@wandb.com for guidance and to report any issues.\n\n==================================================\nGeneral setup has been done!\n==================================================\n\n==================================================\nModel & Tokenizer\n==================================================\nLoading tokenizer and model...\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.9\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/PARADIS-final_project/wandb/run-20250621_182026-f6jv04dx\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mFSDP\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/tqhb2502-hanoi-university-of-science-and-technology/PARADIS-Qwen3_1.7B\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/tqhb2502-hanoi-university-of-science-and-technology/PARADIS-Qwen3_1.7B/runs/f6jv04dx\u001b[0m\nLoading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:06<00:00,  3.45s/it]\nLoading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:06<00:00,  3.36s/it]\nModel loaded. Parameters: 1,720,574,976\nFullyShardedDataParallel(\n  (_fsdp_wrapped_module): Qwen3ForCausalLM(\n    (model): Qwen3Model(\n      (embed_tokens): Embedding(151936, 2048)\n      (layers): ModuleList(\n        (0-27): 28 x FullyShardedDataParallel(\n          (_fsdp_wrapped_module): Qwen3DecoderLayer(\n            (self_attn): Qwen3Attention(\n              (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n              (k_proj): Linear(in_features=2048, out_features=1024, bias=False)\n              (v_proj): Linear(in_features=2048, out_features=1024, bias=False)\n              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n            )\n            (mlp): Qwen3MLP(\n              (gate_proj): Linear(in_features=2048, out_features=6144, bias=False)\n              (up_proj): Linear(in_features=2048, out_features=6144, bias=False)\n              (down_proj): Linear(in_features=6144, out_features=2048, bias=False)\n              (act_fn): SiLU()\n            )\n            (input_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n            (post_attention_layernorm): Qwen3RMSNorm((2048,), eps=1e-06)\n          )\n        )\n      )\n      (norm): Qwen3RMSNorm((2048,), eps=1e-06)\n      (rotary_emb): Qwen3RotaryEmbedding()\n    )\n    (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n  )\n)\n\n==================================================\nDataset\n==================================================\nTotal: 1263196 samples\nTrain samples: 10000\nValid samples: 10000\nTrain batches: 2500\nValid batches: 2500\n\n==================================================\nOptimizer & scheduler\n==================================================\nTotal training steps: 1562\nWarmup steps: 156\n\n==================================================\nEpoch 1/5\n==================================================\nTraining...\nW0621 18:20:57.713000 1044 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 1046 closing signal SIGTERM\nE0621 18:21:01.280000 1044 torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: -9) local_rank: 1 (pid: 1047) of binary: /usr/bin/python3\nTraceback (most recent call last):\n  File \"/usr/local/bin/torchrun\", line 10, in <module>\n    sys.exit(main())\n             ^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\n    return f(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/run.py\", line 918, in main\n    run(args)\n  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/run.py\", line 909, in run\n    elastic_launch(\n  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/launcher/api.py\", line 138, in __call__\n    return launch_agent(self._config, self._entrypoint, list(args))\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/dist-packages/torch/distributed/launcher/api.py\", line 269, in launch_agent\n    raise ChildFailedError(\ntorch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n=====================================================\nPARADIS-Qwen3-FSDP.py FAILED\n-----------------------------------------------------\nFailures:\n  <NO_OTHER_FAILURES>\n-----------------------------------------------------\nRoot Cause (first observed failure):\n[0]:\n  time      : 2025-06-21_18:20:57\n  host      : 609cf74aaf14\n  rank      : 1 (local_rank: 1)\n  exitcode  : -9 (pid: 1047)\n  error_file: <N/A>\n  traceback : Signal 9 (SIGKILL) received by PID 1047\n=====================================================\n","output_type":"stream"}],"execution_count":16}]}