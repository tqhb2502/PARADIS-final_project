{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T10:35:59.775137Z",
     "iopub.status.busy": "2025-06-21T10:35:59.774539Z",
     "iopub.status.idle": "2025-06-21T10:35:59.783093Z",
     "shell.execute_reply": "2025-06-21T10:35:59.782376Z",
     "shell.execute_reply.started": "2025-06-21T10:35:59.775113Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\" # Only use 1 GPU\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T10:35:59.784497Z",
     "iopub.status.busy": "2025-06-21T10:35:59.784297Z",
     "iopub.status.idle": "2025-06-21T10:36:00.005839Z",
     "shell.execute_reply": "2025-06-21T10:36:00.005151Z",
     "shell.execute_reply.started": "2025-06-21T10:35:59.784482Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "HF_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")\n",
    "WANDB_API_KEY = user_secrets.get_secret(\"WANDB_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T10:36:00.007051Z",
     "iopub.status.busy": "2025-06-21T10:36:00.006779Z",
     "iopub.status.idle": "2025-06-21T10:39:19.473159Z",
     "shell.execute_reply": "2025-06-21T10:39:19.472529Z",
     "shell.execute_reply.started": "2025-06-21T10:36:00.007025Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m466.3/466.3 kB\u001b[0m \u001b[31m33.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m103.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m365.3/365.3 kB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m73.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for mpi4py (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install -qU transformers==4.51.0 accelerate bitsandbytes deepspeed mpi4py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-21T10:39:19.475339Z",
     "iopub.status.busy": "2025-06-21T10:39:19.475031Z",
     "iopub.status.idle": "2025-06-21T10:39:43.683656Z",
     "shell.execute_reply": "2025-06-21T10:39:43.682854Z",
     "shell.execute_reply.started": "2025-06-21T10:39:19.475314Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-21 10:39:26,639] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/usr/bin/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-21 10:39:29,890] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-21 10:39:32.478288: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1750502372.700884      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1750502372.788975      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from datasets import load_dataset\n",
    "\n",
    "import wandb\n",
    "import os\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "import gc\n",
    "import math\n",
    "import time\n",
    "import deepspeed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random seed & device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T10:39:43.684982Z",
     "iopub.status.busy": "2025-06-21T10:39:43.684457Z",
     "iopub.status.idle": "2025-06-21T10:39:43.693248Z",
     "shell.execute_reply": "2025-06-21T10:39:43.692437Z",
     "shell.execute_reply.started": "2025-06-21T10:39:43.684960Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T10:39:43.694510Z",
     "iopub.status.busy": "2025-06-21T10:39:43.694190Z",
     "iopub.status.idle": "2025-06-21T10:39:43.872654Z",
     "shell.execute_reply": "2025-06-21T10:39:43.871991Z",
     "shell.execute_reply.started": "2025-06-21T10:39:43.694483Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    # Model configuration\n",
    "    model_name = \"Qwen/Qwen3-0.6B-Instruct\"\n",
    "    # model_name = \"Qwen/Qwen3-1.7B\"\n",
    "    dataset_name = \"vietgpt/wikipedia_vi\"\n",
    "    \n",
    "    # Training configuration\n",
    "    output_dir = \"./qwen-vietnamese-wiki-finetuned\"\n",
    "    # output_dir = \"./qwen-vietnamese-wiki-finetuned-2\"\n",
    "    num_train_epochs = 5\n",
    "    per_device_train_batch_size = 1\n",
    "    per_device_valid_batch_size = 1\n",
    "    gradient_accumulation_steps = 8\n",
    "    learning_rate = 5e-5\n",
    "    weight_decay = 0.01\n",
    "    warmup_ratio = 0.1\n",
    "    max_length = 128\n",
    "\n",
    "    # Optimization settings\n",
    "    adam_epsilon = 1e-8\n",
    "    max_grad_norm = 1.0\n",
    "    \n",
    "    # Logging and saving\n",
    "    logging_steps = 40\n",
    "    save_strategy = \"epoch\"\n",
    "    valid_strategy = \"epoch\"\n",
    "    \n",
    "    # Other settings\n",
    "    fp16 = True\n",
    "    num_workers = os.cpu_count()\n",
    "    \n",
    "    # W&B configuration\n",
    "    use_wandb = True\n",
    "    wandb_run_id = None\n",
    "    wandb_project = \"DeepSpeed-Pipeline-Qwen\"\n",
    "    # wandb_project = \"PARADIS-Qwen3_1.7B\"\n",
    "    wandb_run_name = \"2GPU-qwen3_0.6B\"\n",
    "\n",
    "    # HuggingFace configuration\n",
    "    use_hf = True\n",
    "    hf_repo = \"ThanhND1501/DeepSpeed-Pipeline-Qwen-10kWikiVi\"\n",
    "    # hf_repo = \"h9art/PARADIS-Qwen3_1.7B-10kWikiVi-1GPU\"\n",
    "    \n",
    "    # Dataset\n",
    "    train_size = 10000\n",
    "    valid_size = 10000\n",
    "    test_size = 5000\n",
    "    min_text_length = 50\n",
    "    random_seed = 42\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T10:39:43.873648Z",
     "iopub.status.busy": "2025-06-21T10:39:43.873381Z",
     "iopub.status.idle": "2025-06-21T10:39:43.886261Z",
     "shell.execute_reply": "2025-06-21T10:39:43.885704Z",
     "shell.execute_reply.started": "2025-06-21T10:39:43.873621Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_name': 'Qwen/Qwen2.5-0.5B-Instruct',\n",
       " 'dataset_name': 'vietgpt/wikipedia_vi',\n",
       " 'output_dir': './qwen-vietnamese-wiki-finetuned',\n",
       " 'num_train_epochs': 5,\n",
       " 'per_device_train_batch_size': 1,\n",
       " 'per_device_valid_batch_size': 1,\n",
       " 'gradient_accumulation_steps': 8,\n",
       " 'learning_rate': 5e-05,\n",
       " 'weight_decay': 0.01,\n",
       " 'warmup_ratio': 0.1,\n",
       " 'max_length': 128,\n",
       " 'adam_epsilon': 1e-08,\n",
       " 'max_grad_norm': 1.0,\n",
       " 'logging_steps': 40,\n",
       " 'save_strategy': 'epoch',\n",
       " 'valid_strategy': 'epoch',\n",
       " 'fp16': True,\n",
       " 'num_workers': 4,\n",
       " 'use_wandb': True,\n",
       " 'wandb_run_id': None,\n",
       " 'wandb_project': 'DeepSpeed-Pipeline-Qwen',\n",
       " 'wandb_run_name': '2GPU-qwen2.5_0.5B',\n",
       " 'use_hf': True,\n",
       " 'hf_repo': 'ThanhND1501/DeepSpeed-Pipeline-Qwen-10kWikiVi',\n",
       " 'train_size': 10000,\n",
       " 'valid_size': 10000,\n",
       " 'test_size': 5000,\n",
       " 'min_text_length': 50,\n",
       " 'random_seed': 42}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_dict = {k: v for k, v in Config.__dict__.items() if not k.startswith(\"__\") and not callable(v)}\n",
    "config_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T10:39:43.887261Z",
     "iopub.status.busy": "2025-06-21T10:39:43.887000Z",
     "iopub.status.idle": "2025-06-21T10:39:43.895826Z",
     "shell.execute_reply": "2025-06-21T10:39:43.895103Z",
     "shell.execute_reply.started": "2025-06-21T10:39:43.887238Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ds_config = {\n",
    "    \"train_batch_size\": config.per_device_train_batch_size * config.gradient_accumulation_steps,\n",
    "    \"gradient_accumulation_steps\": config.gradient_accumulation_steps,\n",
    "    \"fp16\": {\n",
    "        \"enabled\": config.fp16\n",
    "    },\n",
    "    \"optimizer\": {\n",
    "        \"type\": \"Adam\",\n",
    "        \"params\": {\n",
    "            \"lr\": config.learning_rate,\n",
    "            \"eps\": config.adam_epsilon,\n",
    "            \"weight_decay\": config.weight_decay\n",
    "        }\n",
    "    },\n",
    "    \"pipeline\": {\n",
    "        \"stages\": 4,\n",
    "        \"partition_method\": \"size\"\n",
    "    },\n",
    "    \"zero_optimization\": {\n",
    "        \"stage\": 2\n",
    "    }\n",
    "}\n",
    "ds_config_path = \"ds_config.json\"\n",
    "with open(ds_config_path, \"w\") as f:\n",
    "    json.dump(ds_config, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T10:39:43.899111Z",
     "iopub.status.busy": "2025-06-21T10:39:43.898862Z",
     "iopub.status.idle": "2025-06-21T10:39:56.280250Z",
     "shell.execute_reply": "2025-06-21T10:39:56.279674Z",
     "shell.execute_reply.started": "2025-06-21T10:39:43.899096Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mthanh_nd\u001b[0m (\u001b[33mthanh_nd_ai\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.9"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20250621_103950-m0wly4aq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thanh_nd_ai/DeepSpeed-Pipeline-Qwen/runs/m0wly4aq' target=\"_blank\">2GPU-qwen2.5_0.5B</a></strong> to <a href='https://wandb.ai/thanh_nd_ai/DeepSpeed-Pipeline-Qwen' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thanh_nd_ai/DeepSpeed-Pipeline-Qwen' target=\"_blank\">https://wandb.ai/thanh_nd_ai/DeepSpeed-Pipeline-Qwen</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thanh_nd_ai/DeepSpeed-Pipeline-Qwen/runs/m0wly4aq' target=\"_blank\">https://wandb.ai/thanh_nd_ai/DeepSpeed-Pipeline-Qwen/runs/m0wly4aq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.login(key=WANDB_API_KEY)\n",
    "if config.use_wandb:\n",
    "    if config.wandb_run_id is None:\n",
    "        wandb.init( # New run\n",
    "            project=config.wandb_project,\n",
    "            name=config.wandb_run_name,\n",
    "            config=config_dict,\n",
    "        )\n",
    "    else:\n",
    "        wandb.init( # Resume to created run\n",
    "            project=config.wandb_project,\n",
    "            id=config.wandb_run_id,\n",
    "            resume='allow',\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T10:39:56.281197Z",
     "iopub.status.busy": "2025-06-21T10:39:56.280996Z",
     "iopub.status.idle": "2025-06-21T10:39:56.417496Z",
     "shell.execute_reply": "2025-06-21T10:39:56.416821Z",
     "shell.execute_reply.started": "2025-06-21T10:39:56.281182Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if config.use_hf:\n",
    "    from huggingface_hub import login, HfApi\n",
    "    login(HF_TOKEN)\n",
    "    hf_api = HfApi()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T10:39:56.418405Z",
     "iopub.status.busy": "2025-06-21T10:39:56.418185Z",
     "iopub.status.idle": "2025-06-21T10:40:03.420858Z",
     "shell.execute_reply": "2025-06-21T10:40:03.420283Z",
     "shell.execute_reply.started": "2025-06-21T10:39:56.418389Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer and model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "537721bf68164c1facf63fb392fe83db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/7.30k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "277b26e342ae40009739a8ff640d2ad6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad0f366979d14280913ab4e80a8ed191",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20b91adc421a4109ab3ff1524f921082",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "461643fdacf844e6a042b71e333b9074",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/659 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ff02984d7a84482a00b39de29434416",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/988M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f1029ba51da4ed4a6a718df6114bbb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded. Parameters: 494,032,768\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading tokenizer and model...\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    config.model_name,\n",
    "    trust_remote_code=True,\n",
    "    padding_side=\"right\"\n",
    ")\n",
    "\n",
    "# Add pad token if it doesn't exist\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Cấu hình 4-bit quantization\n",
    "# quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.model_name,\n",
    "    device_map=\"auto\", # automatically move to correct device\n",
    "    #quantization_config=quantization_config,\n",
    "    torch_dtype=torch.float32,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Turn on gradient checkpointing to save memory\n",
    "model.config.use_cache = False\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# Num parameters\n",
    "print(f\"Model loaded. Parameters: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T10:40:03.421827Z",
     "iopub.status.busy": "2025-06-21T10:40:03.421589Z",
     "iopub.status.idle": "2025-06-21T10:40:03.427514Z",
     "shell.execute_reply": "2025-06-21T10:40:03.426844Z",
     "shell.execute_reply.started": "2025-06-21T10:40:03.421802Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_text(\n",
    "    prompt,\n",
    "    max_length=config.max_length,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    top_k=50\n",
    "):\n",
    "    \"\"\"Generate text using DeepSpeed pipeline-enabled model.\"\"\"\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Tokenize and move to correct device\n",
    "    inputs = tokenizer.encode(prompt, return_tensors='pt').to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.module.generate(\n",
    "            inputs,\n",
    "            max_length=max_length,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            top_k=top_k,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            repetition_penalty=1.1\n",
    "        )\n",
    "\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T10:40:03.428516Z",
     "iopub.status.busy": "2025-06-21T10:40:03.428250Z",
     "iopub.status.idle": "2025-06-21T10:40:04.082615Z",
     "shell.execute_reply": "2025-06-21T10:40:04.081847Z",
     "shell.execute_reply.started": "2025-06-21T10:40:03.428496Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "wikiclass = \"\"\"from torch.utils.data import DataLoader, Dataset\n",
    "class WikiViDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, max_length):\n",
    "        self.dataset = dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get data\n",
    "        item = self.dataset[idx]\n",
    "        combined_text = f\"Tiêu đề: {item['title']}\\\\n\\\\nNội dung: {item['text']}\"\n",
    "\n",
    "        # Tokenize data\n",
    "        tokenized_text = self.tokenizer(\n",
    "            combined_text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        # # Print a tokenized sample\n",
    "        # print(tokenized_text)\n",
    "\n",
    "        # Prepare data from tokenizer output\n",
    "        input_ids = tokenized_text[\"input_ids\"]\n",
    "        attention_mask = tokenized_text[\"attention_mask\"]\n",
    "        labels = input_ids.clone() # In causal LM, labels is the same with input_ids\n",
    "        labels[attention_mask == 0] = -100 # Do not calculate loss on padding tokens\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels,\n",
    "        }\"\"\"\n",
    "with open('wikidataset.py', 'w', encoding='utf-8') as f:\n",
    "    f.write(wikiclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T10:40:04.083742Z",
     "iopub.status.busy": "2025-06-21T10:40:04.083458Z",
     "iopub.status.idle": "2025-06-21T10:40:04.094908Z",
     "shell.execute_reply": "2025-06-21T10:40:04.094391Z",
     "shell.execute_reply.started": "2025-06-21T10:40:04.083720Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from wikidataset import WikiViDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load wikipedia_vi dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T10:40:04.095885Z",
     "iopub.status.busy": "2025-06-21T10:40:04.095603Z",
     "iopub.status.idle": "2025-06-21T10:40:12.712696Z",
     "shell.execute_reply": "2025-06-21T10:40:12.712046Z",
     "shell.execute_reply.started": "2025-06-21T10:40:04.095862Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43be44fecb1d4ee48a9a598fa8f7cf6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/632 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8282ef4686841bc9d1c9b2cb1e5b1aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00000-of-00003-6218d2963e302058.parquet:   0%|          | 0.00/245M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3235457b50764f7abedf88acd5dc0c70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00001-of-00003-12e6c4fadbec91d4.parquet:   0%|          | 0.00/55.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd9f46db78b94ea5a2bc32d2ab9687a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00002-of-00003-175fcfe1c45b0b85.parquet:   0%|          | 0.00/270M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2529d1071c7f473ea20d4549d624f2fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1284930 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded. Total samples: 1284930\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading dataset...\")\n",
    "dataset = load_dataset(config.dataset_name, split=\"train\")\n",
    "print(f\"Dataset loaded. Total samples: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T10:40:12.713834Z",
     "iopub.status.busy": "2025-06-21T10:40:12.713586Z",
     "iopub.status.idle": "2025-06-21T10:40:12.720161Z",
     "shell.execute_reply": "2025-06-21T10:40:12.719550Z",
     "shell.execute_reply.started": "2025-06-21T10:40:12.713816Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 2,\n",
       " 'revid': '90949',\n",
       " 'url': 'https://vi.wikipedia.org/wiki?curid=2',\n",
       " 'title': 'Trang Chính',\n",
       " 'text': '&lt;templatestyles src=\"Wiki2021/styles.css\" /&gt;__NOEDITSECTION__\\n \\n \\n \\n '}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T10:40:12.721025Z",
     "iopub.status.busy": "2025-06-21T10:40:12.720794Z",
     "iopub.status.idle": "2025-06-21T10:40:14.317577Z",
     "shell.execute_reply": "2025-06-21T10:40:14.316698Z",
     "shell.execute_reply.started": "2025-06-21T10:40:12.721010Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# keep only title and text column\n",
    "dataset = dataset.select_columns(['title', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T10:40:14.319446Z",
     "iopub.status.busy": "2025-06-21T10:40:14.318578Z",
     "iopub.status.idle": "2025-06-21T10:40:15.107256Z",
     "shell.execute_reply": "2025-06-21T10:40:15.106479Z",
     "shell.execute_reply.started": "2025-06-21T10:40:14.319412Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Trang Chính',\n",
       " 'text': '&lt;templatestyles src=\"Wiki2021/styles.css\" /&gt;__NOEDITSECTION__\\n \\n \\n \\n '}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T10:40:15.108365Z",
     "iopub.status.busy": "2025-06-21T10:40:15.108090Z",
     "iopub.status.idle": "2025-06-21T10:40:25.190685Z",
     "shell.execute_reply": "2025-06-21T10:40:25.189945Z",
     "shell.execute_reply.started": "2025-06-21T10:40:15.108341Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d24d5a2b3d745719be693ac119c8f17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1284930 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After filtering: 1263196 samples\n"
     ]
    }
   ],
   "source": [
    "def filter_function(example):\n",
    "    \"\"\"Filter out empty or very short texts\"\"\"\n",
    "    \n",
    "    return (\n",
    "        example['text'] is not None and \n",
    "        example['title'] is not None and\n",
    "        len(example['text'].strip()) > config.min_text_length\n",
    "    )\n",
    "\n",
    "dataset = dataset.filter(filter_function)\n",
    "print(f\"After filtering: {len(dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T10:40:25.191875Z",
     "iopub.status.busy": "2025-06-21T10:40:25.191596Z",
     "iopub.status.idle": "2025-06-21T10:40:25.609575Z",
     "shell.execute_reply": "2025-06-21T10:40:25.608991Z",
     "shell.execute_reply.started": "2025-06-21T10:40:25.191852Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train split: 10000 samples\n",
      "valid split: 10000 samples\n",
      "test split: 5000 samples\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.shuffle(seed=config.random_seed)\n",
    "\n",
    "train_split = dataset.select(range(\n",
    "    config.train_size\n",
    "))\n",
    "\n",
    "valid_split = dataset.select(range(\n",
    "    config.train_size,\n",
    "    config.train_size + config.valid_size\n",
    "))\n",
    "\n",
    "test_split = dataset.select(range(\n",
    "    config.train_size + config.valid_size,\n",
    "    config.train_size + config.valid_size + config.test_size\n",
    "))\n",
    "\n",
    "print(f'train split: {len(train_split)} samples')\n",
    "print(f'valid split: {len(valid_split)} samples')\n",
    "print(f'test split: {len(test_split)} samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T10:40:25.610531Z",
     "iopub.status.busy": "2025-06-21T10:40:25.610291Z",
     "iopub.status.idle": "2025-06-21T10:40:28.590691Z",
     "shell.execute_reply": "2025-06-21T10:40:28.589940Z",
     "shell.execute_reply.started": "2025-06-21T10:40:25.610507Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_ds = WikiViDataset(train_split, tokenizer, config.max_length)\n",
    "valid_ds = WikiViDataset(valid_split, tokenizer, config.max_length)\n",
    "test_ds = WikiViDataset(test_split, tokenizer, config.max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T10:40:28.591968Z",
     "iopub.status.busy": "2025-06-21T10:40:28.591641Z",
     "iopub.status.idle": "2025-06-21T10:40:28.601040Z",
     "shell.execute_reply": "2025-06-21T10:40:28.600338Z",
     "shell.execute_reply.started": "2025-06-21T10:40:28.591941Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Display a sample\n",
    "# train_ds[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T10:40:28.602204Z",
     "iopub.status.busy": "2025-06-21T10:40:28.601966Z",
     "iopub.status.idle": "2025-06-21T10:40:28.610712Z",
     "shell.execute_reply": "2025-06-21T10:40:28.610038Z",
     "shell.execute_reply.started": "2025-06-21T10:40:28.602181Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=config.per_device_train_batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=config.num_workers,\n",
    "    pin_memory=False,\n",
    ")\n",
    "\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_ds,\n",
    "    batch_size=config.per_device_valid_batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=config.num_workers,\n",
    "    pin_memory=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T10:40:28.613839Z",
     "iopub.status.busy": "2025-06-21T10:40:28.613638Z",
     "iopub.status.idle": "2025-06-21T10:40:28.620968Z",
     "shell.execute_reply": "2025-06-21T10:40:28.620373Z",
     "shell.execute_reply.started": "2025-06-21T10:40:28.613824Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 10000\n",
      "Valid batches: 10000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train batches: {len(train_dataloader)}\")\n",
    "print(f\"Valid batches: {len(valid_dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer & scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T10:40:28.622016Z",
     "iopub.status.busy": "2025-06-21T10:40:28.621784Z",
     "iopub.status.idle": "2025-06-21T10:40:28.634187Z",
     "shell.execute_reply": "2025-06-21T10:40:28.633599Z",
     "shell.execute_reply.started": "2025-06-21T10:40:28.622001Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training steps: 6250\n",
      "Warmup steps: 625\n"
     ]
    }
   ],
   "source": [
    "total_steps = len(train_dataloader) * config.num_train_epochs // config.gradient_accumulation_steps\n",
    "warmup_steps = int(total_steps * config.warmup_ratio)\n",
    "\n",
    "print(f\"Total training steps: {total_steps}\")\n",
    "print(f\"Warmup steps: {warmup_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T10:43:14.447177Z",
     "iopub.status.busy": "2025-06-21T10:43:14.446535Z",
     "iopub.status.idle": "2025-06-21T10:43:17.267756Z",
     "shell.execute_reply": "2025-06-21T10:43:17.267043Z",
     "shell.execute_reply.started": "2025-06-21T10:43:14.447158Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-21 10:43:14,448] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed info: version=0.17.1, git-hash=unknown, git-branch=unknown\n",
      "[2025-06-21 10:43:14,450] [INFO] [comm.py:700:init_distributed] Distributed backend already initialized\n",
      "[2025-06-21 10:43:14,451] [INFO] [config.py:655:__init__] Config mesh_device None world_size = 1\n",
      "[2025-06-21 10:43:14,465] [INFO] [engine.py:1325:_configure_distributed_model] ********** distributed groups summary **********\n",
      "\t self.dp_world_size=1\n",
      "\t self.mp_world_size=1\n",
      "\t self.seq_dp_world_size=1\n",
      "\t self.sequence_parallel_size=1\n",
      "***********************************************\n",
      "[2025-06-21 10:43:14,480] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2025-06-21 10:43:14,482] [INFO] [logging.py:107:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer\n",
      "[2025-06-21 10:43:14,483] [INFO] [logging.py:107:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2025-06-21 10:43:14,495] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam\n",
      "[2025-06-21 10:43:14,496] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>\n",
      "[2025-06-21 10:43:14,497] [INFO] [logging.py:107:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 2 optimizer\n",
      "[2025-06-21 10:43:14,498] [INFO] [stage_1_and_2.py:151:__init__] Reduce bucket size 500000000\n",
      "[2025-06-21 10:43:14,498] [INFO] [stage_1_and_2.py:152:__init__] Allgather bucket size 500000000\n",
      "[2025-06-21 10:43:14,499] [INFO] [stage_1_and_2.py:153:__init__] CPU Offload: False\n",
      "[2025-06-21 10:43:14,500] [INFO] [stage_1_and_2.py:154:__init__] Round robin gradient partitioning: False\n",
      "[2025-06-21 10:43:16,296] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states\n",
      "[2025-06-21 10:43:16,298] [INFO] [utils.py:782:see_memory_usage] MA 5.52 GB         Max_MA 6.44 GB         CA 6.45 GB         Max_CA 6 GB \n",
      "[2025-06-21 10:43:16,299] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 5.02 GB, percent = 16.0%\n",
      "[2025-06-21 10:43:16,738] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states\n",
      "[2025-06-21 10:43:16,740] [INFO] [utils.py:782:see_memory_usage] MA 5.52 GB         Max_MA 7.36 GB         CA 8.29 GB         Max_CA 8 GB \n",
      "[2025-06-21 10:43:16,742] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 5.02 GB, percent = 16.0%\n",
      "[2025-06-21 10:43:16,744] [INFO] [stage_1_and_2.py:573:__init__] optimizer state initialized\n",
      "[2025-06-21 10:43:17,174] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2025-06-21 10:43:17,176] [INFO] [utils.py:782:see_memory_usage] MA 5.52 GB         Max_MA 5.52 GB         CA 8.29 GB         Max_CA 8 GB \n",
      "[2025-06-21 10:43:17,177] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 5.02 GB, percent = 16.0%\n",
      "[2025-06-21 10:43:17,185] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer\n",
      "[2025-06-21 10:43:17,186] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None\n",
      "[2025-06-21 10:43:17,187] [INFO] [logging.py:107:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2025-06-21 10:43:17,188] [INFO] [logging.py:107:log_dist] [Rank 0] step=0, skipped=0, lr=[5e-05], mom=[(0.9, 0.999)]\n",
      "[2025-06-21 10:43:17,190] [INFO] [logging.py:107:log_dist] [Rank 0] [TorchCheckpointEngine] Initialized with serialization = True\n",
      "[2025-06-21 10:43:17,191] [INFO] [config.py:921:print] DeepSpeedEngine configuration:\n",
      "[2025-06-21 10:43:17,193] [INFO] [config.py:925:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2025-06-21 10:43:17,193] [INFO] [config.py:925:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}\n",
      "[2025-06-21 10:43:17,194] [INFO] [config.py:925:print]   amp_enabled .................. False\n",
      "[2025-06-21 10:43:17,195] [INFO] [config.py:925:print]   amp_params ................... False\n",
      "[2025-06-21 10:43:17,197] [INFO] [config.py:925:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2025-06-21 10:43:17,198] [INFO] [config.py:925:print]   bfloat16_config .............. enabled=False immediate_grad_update=False check_grad_overflow=False\n",
      "[2025-06-21 10:43:17,199] [INFO] [config.py:925:print]   checkpoint_config ............ {'tag_validation': 'WARN', 'checkpoint_serialization': True, 'writer': None}\n",
      "[2025-06-21 10:43:17,200] [INFO] [config.py:925:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2025-06-21 10:43:17,200] [INFO] [config.py:925:print]   checkpoint_tag_validation_enabled  True\n",
      "[2025-06-21 10:43:17,201] [INFO] [config.py:925:print]   checkpoint_tag_validation_fail  False\n",
      "[2025-06-21 10:43:17,202] [INFO] [config.py:925:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f5a18213a50>\n",
      "[2025-06-21 10:43:17,203] [INFO] [config.py:925:print]   communication_data_type ...... None\n",
      "[2025-06-21 10:43:17,204] [INFO] [config.py:925:print]   compile_config ............... deepcompile=False free_activation=False offload_activation=False offload_opt_states=False double_buffer=True symmetric_memory=False debug_log=False offload_parameters=False sync_before_reduce=False sync_after_reduce=False sync_before_allgather=False sync_after_allgather=False\n",
      "[2025-06-21 10:43:17,205] [INFO] [config.py:925:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2025-06-21 10:43:17,206] [INFO] [config.py:925:print]   curriculum_enabled_legacy .... False\n",
      "[2025-06-21 10:43:17,207] [INFO] [config.py:925:print]   curriculum_params_legacy ..... False\n",
      "[2025-06-21 10:43:17,208] [INFO] [config.py:925:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'pin_memory': False, 'curriculum_learning': {'enabled': False}, 'dynamic_batching': {'enabled': False, 'lr_scaling_method': 'linear', 'min_batch_size': 1, 'max_batch_size': None, 'sequence_picking_order': 'dataloader', 'verbose': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2025-06-21 10:43:17,209] [INFO] [config.py:925:print]   data_efficiency_enabled ...... False\n",
      "[2025-06-21 10:43:17,210] [INFO] [config.py:925:print]   dataloader_drop_last ......... False\n",
      "[2025-06-21 10:43:17,211] [INFO] [config.py:925:print]   disable_allgather ............ False\n",
      "[2025-06-21 10:43:17,212] [INFO] [config.py:925:print]   dump_state ................... False\n",
      "[2025-06-21 10:43:17,213] [INFO] [config.py:925:print]   eigenvalue_enabled ........... False\n",
      "[2025-06-21 10:43:17,214] [INFO] [config.py:925:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2025-06-21 10:43:17,215] [INFO] [config.py:925:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2025-06-21 10:43:17,216] [INFO] [config.py:925:print]   eigenvalue_layer_num ......... 0\n",
      "[2025-06-21 10:43:17,217] [INFO] [config.py:925:print]   eigenvalue_max_iter .......... 100\n",
      "[2025-06-21 10:43:17,218] [INFO] [config.py:925:print]   eigenvalue_stability ......... 1e-06\n",
      "[2025-06-21 10:43:17,219] [INFO] [config.py:925:print]   eigenvalue_tol ............... 0.01\n",
      "[2025-06-21 10:43:17,220] [INFO] [config.py:925:print]   eigenvalue_verbose ........... False\n",
      "[2025-06-21 10:43:17,221] [INFO] [config.py:925:print]   elasticity_enabled ........... False\n",
      "[2025-06-21 10:43:17,222] [INFO] [config.py:925:print]   float16_config ............... enabled=True auto_cast=False loss_scale=0.0 initial_scale_power=16 loss_scale_window=1000 hysteresis=2 consecutive_hysteresis=False min_loss_scale=1 fp16_master_weights_and_grads=False\n",
      "[2025-06-21 10:43:17,223] [INFO] [config.py:925:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2025-06-21 10:43:17,224] [INFO] [config.py:925:print]   global_rank .................. 0\n",
      "[2025-06-21 10:43:17,224] [INFO] [config.py:925:print]   grad_accum_dtype ............. None\n",
      "[2025-06-21 10:43:17,226] [INFO] [config.py:925:print]   gradient_accumulation_steps .. 8\n",
      "[2025-06-21 10:43:17,227] [INFO] [config.py:925:print]   gradient_clipping ............ 0.0\n",
      "[2025-06-21 10:43:17,228] [INFO] [config.py:925:print]   gradient_predivide_factor .... 1.0\n",
      "[2025-06-21 10:43:17,229] [INFO] [config.py:925:print]   graph_harvesting ............. False\n",
      "[2025-06-21 10:43:17,230] [INFO] [config.py:925:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2025-06-21 10:43:17,231] [INFO] [config.py:925:print]   load_universal_checkpoint .... False\n",
      "[2025-06-21 10:43:17,232] [INFO] [config.py:925:print]   memory_breakdown ............. False\n",
      "[2025-06-21 10:43:17,233] [INFO] [config.py:925:print]   mics_hierarchial_params_gather  False\n",
      "[2025-06-21 10:43:17,234] [INFO] [config.py:925:print]   mics_shard_size .............. -1\n",
      "[2025-06-21 10:43:17,235] [INFO] [config.py:925:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')\n",
      "[2025-06-21 10:43:17,236] [INFO] [config.py:925:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2025-06-21 10:43:17,237] [INFO] [config.py:925:print]   optimizer_legacy_fusion ...... False\n",
      "[2025-06-21 10:43:17,238] [INFO] [config.py:925:print]   optimizer_name ............... adam\n",
      "[2025-06-21 10:43:17,239] [INFO] [config.py:925:print]   optimizer_params ............. {'lr': 5e-05, 'eps': 1e-08, 'weight_decay': 0.01}\n",
      "[2025-06-21 10:43:17,240] [INFO] [config.py:925:print]   pipeline ..................... {'stages': 4, 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True, 'partition_method': 'size'}\n",
      "[2025-06-21 10:43:17,241] [INFO] [config.py:925:print]   pld_enabled .................. False\n",
      "[2025-06-21 10:43:17,242] [INFO] [config.py:925:print]   pld_params ................... False\n",
      "[2025-06-21 10:43:17,243] [INFO] [config.py:925:print]   prescale_gradients ........... False\n",
      "[2025-06-21 10:43:17,244] [INFO] [config.py:925:print]   scheduler_name ............... None\n",
      "[2025-06-21 10:43:17,245] [INFO] [config.py:925:print]   scheduler_params ............. None\n",
      "[2025-06-21 10:43:17,246] [INFO] [config.py:925:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2025-06-21 10:43:17,247] [INFO] [config.py:925:print]   sparse_attention ............. None\n",
      "[2025-06-21 10:43:17,248] [INFO] [config.py:925:print]   sparse_gradients_enabled ..... False\n",
      "[2025-06-21 10:43:17,249] [INFO] [config.py:925:print]   steps_per_print .............. None\n",
      "[2025-06-21 10:43:17,250] [INFO] [config.py:925:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tp_overlap_comm=False tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False\n",
      "[2025-06-21 10:43:17,251] [INFO] [config.py:925:print]   timers_config ................ enabled=True synchronized=True\n",
      "[2025-06-21 10:43:17,251] [INFO] [config.py:925:print]   train_batch_size ............. 8\n",
      "[2025-06-21 10:43:17,253] [INFO] [config.py:925:print]   train_micro_batch_size_per_gpu  1\n",
      "[2025-06-21 10:43:17,254] [INFO] [config.py:925:print]   use_data_before_expert_parallel_  False\n",
      "[2025-06-21 10:43:17,254] [INFO] [config.py:925:print]   use_node_local_storage ....... False\n",
      "[2025-06-21 10:43:17,255] [INFO] [config.py:925:print]   wall_clock_breakdown ......... False\n",
      "[2025-06-21 10:43:17,256] [INFO] [config.py:925:print]   weight_quantization_config ... None\n",
      "[2025-06-21 10:43:17,257] [INFO] [config.py:925:print]   world_size ................... 1\n",
      "[2025-06-21 10:43:17,258] [INFO] [config.py:925:print]   zero_allow_untested_optimizer  False\n",
      "[2025-06-21 10:43:17,260] [INFO] [config.py:925:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False\n",
      "[2025-06-21 10:43:17,261] [INFO] [config.py:925:print]   zero_enabled ................. True\n",
      "[2025-06-21 10:43:17,262] [INFO] [config.py:925:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2025-06-21 10:43:17,263] [INFO] [config.py:925:print]   zero_optimization_stage ...... 2\n",
      "[2025-06-21 10:43:17,264] [INFO] [config.py:911:print_user_config]   json = {\n",
      "    \"train_batch_size\": 8, \n",
      "    \"gradient_accumulation_steps\": 8, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": true\n",
      "    }, \n",
      "    \"optimizer\": {\n",
      "        \"type\": \"Adam\", \n",
      "        \"params\": {\n",
      "            \"lr\": 5e-05, \n",
      "            \"eps\": 1e-08, \n",
      "            \"weight_decay\": 0.01\n",
      "        }\n",
      "    }, \n",
      "    \"pipeline\": {\n",
      "        \"stages\": 4, \n",
      "        \"partition_method\": \"size\"\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Initialize DeepSpeed engine\n",
    "model, optimizer, _, scheduler = deepspeed.initialize(\n",
    "    model=model,\n",
    "    config_params=ds_config,\n",
    "    dist_init_required=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T10:43:30.164794Z",
     "iopub.status.busy": "2025-06-21T10:43:30.164194Z",
     "iopub.status.idle": "2025-06-21T10:43:30.172483Z",
     "shell.execute_reply": "2025-06-21T10:43:30.171675Z",
     "shell.execute_reply.started": "2025-06-21T10:43:30.164773Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, scheduler, epoch):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    model.zero_grad()\n",
    "    progress_bar = tqdm(dataloader, desc=f\"Training Epoch {epoch + 1}\")\n",
    "\n",
    "    for step, batch in enumerate(progress_bar):\n",
    "        input_ids = batch['input_ids']  # REMOVE .to(model.device)\n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "        #print(input_ids.shape)\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "        loss = outputs.loss / config.gradient_accumulation_steps\n",
    "\n",
    "        model.backward(loss)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if (step + 1) % config.gradient_accumulation_steps == 0:\n",
    "            model.step()\n",
    "            model.zero_grad()\n",
    "\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f\"{loss.item() * config.gradient_accumulation_steps:.4f}\",\n",
    "            'lr': f\"{scheduler.get_last_lr()[0]:.2e}\"\n",
    "        })\n",
    "\n",
    "        if (step + 1) % config.logging_steps == 0:\n",
    "            avg_loss = total_loss / (step + 1) * config.gradient_accumulation_steps\n",
    "            print(f\"Step {step + 1}/{len(dataloader)}, Loss: {avg_loss:.4f}, LR: {scheduler.get_last_lr()[0]:.2e}\")\n",
    "\n",
    "            if config.use_wandb:\n",
    "                wandb.log({\n",
    "                    \"train_loss\": avg_loss,\n",
    "                    \"learning_rate\": scheduler.get_last_lr()[0],\n",
    "                    \"train_step\": epoch * len(dataloader) + step + 1\n",
    "                })\n",
    "\n",
    "    return total_loss / len(dataloader) * config.gradient_accumulation_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T10:43:33.439028Z",
     "iopub.status.busy": "2025-06-21T10:43:33.438761Z",
     "iopub.status.idle": "2025-06-21T10:43:33.445328Z",
     "shell.execute_reply": "2025-06-21T10:43:33.444603Z",
     "shell.execute_reply.started": "2025-06-21T10:43:33.439007Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def validate(model, dataloader):\n",
    "    \"\"\"Validate the model.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_steps = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(dataloader, desc=\"Validating\")\n",
    "        for batch in progress_bar:\n",
    "            input_ids = batch['input_ids']\n",
    "            attention_mask = batch['attention_mask']\n",
    "            labels = batch['labels']\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "            total_steps += 1\n",
    "\n",
    "            progress_bar.set_postfix({'valid_loss': f\"{loss.item():.4f}\"})\n",
    "\n",
    "    avg_loss = total_loss / total_steps\n",
    "    perplexity = math.exp(avg_loss)\n",
    "\n",
    "    return avg_loss, perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T10:41:26.423680Z",
     "iopub.status.busy": "2025-06-21T10:41:26.423450Z",
     "iopub.status.idle": "2025-06-21T10:41:26.434926Z",
     "shell.execute_reply": "2025-06-21T10:41:26.434269Z",
     "shell.execute_reply.started": "2025-06-21T10:41:26.423665Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#test_prompts = [\n",
    "#    \"Việt Nam là một quốc gia\",\n",
    "#    \"Tiêu đề: Hà Nội\\n\\nNội dung:\",\n",
    "#    \"Lịch sử Việt Nam bắt đầu từ\",\n",
    "#    \"Văn hóa truyền thống của người Việt\",\n",
    "#    \"Tiêu đề: Phở\\n\\nNội dung: Phở là\"\n",
    "#]\n",
    "\n",
    "#print(\"\\n\" + \"=\" * 50)\n",
    "#print(\"TESTING THE ORIGINAL MODEL\")\n",
    "#print(\"=\" * 50)\n",
    "\n",
    "#for i, prompt in enumerate(test_prompts, 1):\n",
    "#    print(f\"\\n--- Test {i} ---\")\n",
    "#    print(f\"Prompt: {prompt}\")\n",
    "#    print(\"-\" * 40)\n",
    "#    \n",
    "#    generated = generate_text(prompt, max_length=150, temperature=0.7)\n",
    "#    print(f\"Generated: {generated}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-21T10:43:37.768411Z",
     "iopub.status.busy": "2025-06-21T10:43:37.767727Z",
     "iopub.status.idle": "2025-06-21T10:44:04.323554Z",
     "shell.execute_reply": "2025-06-21T10:44:04.322074Z",
     "shell.execute_reply.started": "2025-06-21T10:43:37.768387Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "\n",
      "==================================================\n",
      "Epoch 1/5\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e08228a5c6741f2b02527fa668e0683",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 1:   0%|          | 0/10000 [00:12<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_35/121116172.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_35/721106599.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, dataloader, optimizer, scheduler, epoch)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m#print(input_ids.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         outputs = model(\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/deepspeed/utils/nvtx.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0menable_nvtx\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_compiling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mget_accelerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrange_push\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__qualname__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mret_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0menable_nvtx\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_compiling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mget_accelerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrange_pop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/deepspeed/runtime/engine.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2085\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch_compile_passes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2086\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2087\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2088\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2089\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautotuning_profile_model_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1844\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1845\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1846\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m             \u001b[0;31m# run always called hooks if they have not already been run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36minner\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1791\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1792\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1793\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1794\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1795\u001b[0m                 for hook_id, hook in (\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_requested_to_return_tuple\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_configured_to_return_tuple\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_top_level_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         outputs: BaseModelOutputWithPast = self.model(\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_requested_to_return_tuple\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_configured_to_return_tuple\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_top_level_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_checkpointing\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m                 layer_outputs = self._gradient_checkpointing_func(\n\u001b[0m\u001b[1;32m    538\u001b[0m                     \u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mflash_attn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_compile.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dynamo_disable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisable_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdisable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    743\u001b[0m             )\n\u001b[1;32m    744\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 745\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    746\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m                 \u001b[0m_maybe_set_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py\u001b[0m in \u001b[0;36mcheckpoint\u001b[0;34m(function, use_reentrant, context_fn, determinism_check, debug, *args, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m                 \u001b[0;34m\"use_reentrant=False.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m             )\n\u001b[0;32m--> 489\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mCheckpointFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreserve\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m         gen = _checkpoint_without_reentrant_generator(\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0;31m# See NOTE: [functorch vjp and autograd interaction]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_functorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap_dead_wrappers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_setup_ctx_defined\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, run_function, preserve_rng_state, *args)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;31m# Self Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         hidden_states, self_attn_weights = self.self_attn(\n\u001b[0m\u001b[1;32m    263\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/qwen2/modeling_qwen2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    192\u001b[0m                 \u001b[0mattention_interface\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mALL_ATTENTION_FUNCTIONS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_attn_implementation\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         attn_output, attn_weights = attention_interface(\n\u001b[0m\u001b[1;32m    195\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0mquery_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/integrations/sdpa_attention.py\u001b[0m in \u001b[0;36msdpa_attention_forward\u001b[0;34m(module, query, key, value, attention_mask, dropout, scaling, is_causal, **kwargs)\u001b[0m\n\u001b[1;32m     28\u001b[0m ) -> Tuple[torch.Tensor, None]:\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"num_key_value_groups\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrepeat_kv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_key_value_groups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrepeat_kv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_key_value_groups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/integrations/sdpa_attention.py\u001b[0m in \u001b[0;36mrepeat_kv\u001b[0;34m(hidden_states, n_rep)\u001b[0m\n\u001b[1;32m      9\u001b[0m     num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n\u001b[1;32m     10\u001b[0m     \"\"\"\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_key_value_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn_rep\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "print(\"Starting training...\")\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(config.output_dir, exist_ok=True)\n",
    "\n",
    "# Training history\n",
    "training_history = {\n",
    "    'train_losses': [],\n",
    "    'train_times': [],\n",
    "    'valid_losses': [],\n",
    "    'valid_perplexities': [],\n",
    "    'valid_times': [],\n",
    "    'learning_rates': []\n",
    "}\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "step_count = 0\n",
    "\n",
    "for epoch in range(config.num_train_epochs):\n",
    "    print(f\"\\n{'=' * 50}\")\n",
    "    print(f\"Epoch {epoch + 1}/{config.num_train_epochs}\")\n",
    "    print(f\"{'=' * 50}\")\n",
    "    \n",
    "    # Training\n",
    "    start_time = time.time()\n",
    "    train_loss = train_epoch(model, train_dataloader, optimizer, scheduler, epoch)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    elapsed_time = end_time - start_time\n",
    "    train_mins, train_secs = divmod(elapsed_time, 60)\n",
    "    training_history['train_times'].append(train_mins)\n",
    "    print(f\"Training Time: {int(train_mins)} mins {int(train_secs)} seconds\")\n",
    "    \n",
    "    training_history['train_losses'].append(train_loss)\n",
    "    print(f\"Training Loss: {train_loss:.4f}\")\n",
    "    \n",
    "    # Validation\n",
    "    start_time = time.time()\n",
    "    valid_loss, perplexity = validate(model, valid_dataloader)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    elapsed_time = end_time - start_time\n",
    "    valid_mins, valid_secs = divmod(elapsed_time, 60)\n",
    "    training_history['valid_times'].append(valid_mins)\n",
    "    print(f\"Training Time: {int(valid_mins)} mins {int(valid_secs)} seconds\")\n",
    "    \n",
    "    training_history['valid_losses'].append(valid_loss)\n",
    "    training_history['valid_perplexities'].append(perplexity)\n",
    "    print(f\"Validation Loss: {valid_loss:.4f}\")\n",
    "    print(f\"Perplexity: {perplexity:.2f}\")\n",
    "    \n",
    "    # Log to wandb\n",
    "    if config.use_wandb:\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"train_time (m)\": train_mins,\n",
    "            \"valid_time (m)\": valid_mins,\n",
    "            \"valid_loss\": valid_loss,\n",
    "            \"perplexity\": perplexity,\n",
    "        })\n",
    "    \n",
    "    # Save best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        \n",
    "        model.save_pretrained(config.output_dir)\n",
    "        tokenizer.save_pretrained(config.output_dir)\n",
    "        print(f\"New best model! Saved to {config.output_dir}\")\n",
    "        \n",
    "        if config.use_hf:\n",
    "            model.push_to_hub(config.hf_repo)\n",
    "            tokenizer.push_to_hub(config.hf_repo)\n",
    "            print(f\"Also saved to repo {config.hf_repo}\")\n",
    "        \n",
    "    # Save training state\n",
    "    torch.save({\n",
    "        'epoch': epoch + 1,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'best_valid_loss': best_valid_loss,\n",
    "        'training_history': training_history\n",
    "    }, os.path.join(config.output_dir, 'training_state.pt'))\n",
    "    print(f\"Training state saved to {config.output_dir}!\")\n",
    "\n",
    "    if config.use_hf:\n",
    "        hf_api.upload_file(\n",
    "            path_or_fileobj=os.path.join(config.output_dir, 'training_state.pt'),\n",
    "            path_in_repo=\"training_state.pt\",\n",
    "            repo_id=config.hf_repo,\n",
    "            repo_type=\"model\",\n",
    "        )\n",
    "    print(f\"Training state pushed to repo {config.hf_repo}!\")\n",
    "    \n",
    "    # Clean up GPU memory\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_prompts = [\n",
    "    \"Việt Nam là một quốc gia\",\n",
    "    \"Tiêu đề: Hà Nội\\n\\nNội dung:\",\n",
    "    \"Lịch sử Việt Nam bắt đầu từ\",\n",
    "    \"Văn hóa truyền thống của người Việt\",\n",
    "    \"Tiêu đề: Phở\\n\\nNội dung: Phở là\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"TESTING THE FINE-TUNED MODEL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"\\n--- Test {i} ---\")\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    generated = generate_text(prompt, max_length=150, temperature=0.7)\n",
    "    print(f\"Generated: {generated}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save training log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save comprehensive training log\n",
    "training_log = {\n",
    "    'config': vars(config),\n",
    "    'model_info': {\n",
    "        'model_name': config.model_name,\n",
    "        'num_parameters': model.module.num_parameters(),\n",
    "        'dataset_name': config.dataset_name,\n",
    "        'train_samples': len(train_ds),\n",
    "        'valid_samples': len(valid_ds)\n",
    "    },\n",
    "    'training_results': {\n",
    "        'best_valid_loss': best_valid_loss,\n",
    "        'final_perplexity': training_history['valid_perplexities'][-1],\n",
    "        'total_epochs': config.num_train_epochs,\n",
    "        'total_steps': total_steps\n",
    "    },\n",
    "    'training_history': training_history,\n",
    "    'training_date': datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "log_path = os.path.join(config.output_dir, 'training_log.json')\n",
    "with open(log_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(training_log, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\nTraining log saved to {log_path}\")\n",
    "\n",
    "if config.use_hf:\n",
    "    hf_api.upload_file(\n",
    "        path_or_fileobj=log_path,\n",
    "        path_in_repo=\"training_log.json\",\n",
    "        repo_id=config.hf_repo,\n",
    "        repo_type=\"model\",\n",
    "    )\n",
    "    print(f\"\\nTraining log pushed to repo {config.hf_repo}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if config.use_wandb:\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !deepspeed --num_gpus 1 train.py --deepspeed ds_config_pipeline.json"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
