{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q transformers datasets deepspeed torch accelerate evaluate bitsandbytes pyyaml wandb","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nWANDB_API_KEY = user_secrets.get_secret(\"WANDB_API_KEY\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import wandb\nwandb.login(key=WANDB_API_KEY)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport yaml\nimport json\n\n# --- Tạo thư mục cấu hình cho Accelerate ---\naccelerate_config_dir = os.path.expanduser(\"~/.cache/huggingface/accelerate\")\nos.makedirs(accelerate_config_dir, exist_ok=True)\n\n# --- Định nghĩa cấu hình Accelerate ---\naccelerate_config = {\n    'compute_environment': 'LOCAL_MACHINE',\n    'distributed_type': 'DEEPSPEED',\n    'deepspeed_config': {\n        'deepspeed_config_file': '/kaggle/working/ds_zero3_config.json',\n        'zero3_init_flag': True, \n    },\n    'machine_rank': 0,\n    'main_training_function': 'main',\n    'num_machines': 1,\n    'num_processes': 2,\n    'use_cpu': False,\n}\n\n# --- Ghi file cấu hình Accelerate ---\nconfig_path = os.path.join(accelerate_config_dir, \"default_config.yaml\")\nwith open(config_path, 'w') as f:\n    yaml.dump(accelerate_config, f, default_flow_style=False)\nprint(f\"File cấu hình Accelerate đã được tạo tại: {config_path}\")\n\n# --- Định nghĩa cấu hình DeepSpeed ---\ndeepspeed_config = {\n  \"fp16\": {\n    \"enabled\": \"auto\"\n  },\n  \"bf16\": {\n    \"enabled\": \"auto\"\n  },\n  \"optimizer\": {\n    \"type\": \"AdamW\",\n    \"params\": {\n      \"lr\": 3e-6,\n      \"betas\": [0.9, 0.999],\n      \"eps\": 1e-8,\n      \"weight_decay\": 0.01\n    }\n  },\n  \"scheduler\": {\n    \"type\": \"WarmupDecayLR\",\n    \"params\": {\n      \"total_num_steps\": 183,\n      \"warmup_min_lr\": 0,\n      \"warmup_max_lr\": 3e-6,\n      \"warmup_num_steps\": 20\n    }\n  },\n  \"zero_optimization\": {\n    \"stage\": 3,\n    \"offload_optimizer\": { \"device\": \"cpu\", \"pin_memory\": True },\n    \"offload_param\": { \"device\": \"cpu\", \"pin_memory\": True },\n    \"overlap_comm\": True,\n    \"contiguous_gradients\": True,\n    \"sub_group_size\": 1e9,\n    \"reduce_bucket_size\": \"auto\",\n    \"stage3_prefetch_bucket_size\": \"auto\",\n    \"stage3_param_persistence_threshold\": \"auto\",\n    \"stage3_max_live_parameters\": 1e9,\n    \"stage3_max_reuse_distance\": 1e9,\n    \"stage3_gather_16bit_weights_on_model_save\": True\n  },\n  \"gradient_accumulation_steps\": \"auto\",\n  \"gradient_clipping\": \"auto\",\n  \"steps_per_print\": 2000,\n  \"train_batch_size\": \"auto\",\n  \"train_micro_batch_size_per_gpu\": \"auto\",\n  \"wall_clock_breakdown\": False\n}\n\n# --- Ghi file cấu hình DeepSpeed ---\nds_config_path = \"/kaggle/working/ds_zero3_config.json\"\nwith open(ds_config_path, 'w') as f:\n    json.dump(deepspeed_config, f, indent=2)\nprint(f\"File cấu hình DeepSpeed đã được sửa và tạo tại: {ds_config_path}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile train_deepspeed_wandb.py\n\nimport torch\nimport time\nimport math\nimport argparse\nimport wandb\nfrom accelerate.utils import DummyOptim, DummyScheduler\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, default_data_collator\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader\nfrom accelerate import Accelerator\nfrom tqdm.auto import tqdm\n\ndef set_seed(seed):\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\ndef evaluate_model(model, dataloader, accelerator, args):\n    model.eval()\n    losses = []\n    eval_start_time = time.time()\n    for batch in dataloader:\n        with torch.no_grad():\n            outputs = model(**batch)\n        loss = outputs.loss\n        losses.append(accelerator.gather_for_metrics(loss.repeat(args.batch_size)))\n\n    losses = torch.cat(losses)\n    if accelerator.num_processes > 1:\n        losses = losses[:len(dataloader.dataset)]\n    try:\n        eval_loss = torch.mean(losses)\n        perplexity = math.exp(eval_loss)\n    except OverflowError:\n        eval_loss = torch.tensor(float(\"inf\"))\n        perplexity = float(\"inf\")\n    \n    eval_time = time.time() - eval_start_time\n    model.train()\n    return eval_loss.item(), perplexity, eval_time\n\ndef main():\n    # Command arguments\n    parser = argparse.ArgumentParser(description=\"Finetune BLOOM with DeepSpeed ZeRO-3 and W&B\")\n    \n    parser.add_argument(\"--model_name\", type=str, default=\"bigscience/bloom-560m\")\n    parser.add_argument(\"--dataset_name\", type=str, default=\"Salesforce/wikitext\")\n    parser.add_argument(\"--dataset_config\", type=str, default=\"wikitext-2-raw-v1\")\n    \n    parser.add_argument(\"--seed\", type=int, default=42)\n    parser.add_argument(\"--num_epochs\", type=int, default=2)\n    parser.add_argument(\"--batch_size\", type=int, default=4)\n    parser.add_argument(\"--block_size\", type=int, default=128)\n    \n    parser.add_argument(\"--logging_steps\", type=int, default=5)\n    parser.add_argument(\"--eval_steps\", type=int, default=20)\n    parser.add_argument(\"--wandb_project\", type=str, default=\"deepspeed_bloom_finetune\")\n    parser.add_argument(\"--wandb_run_name\", type=str, default=None)\n    \n    args = parser.parse_args()\n    \n    if args.wandb_run_name:\n        args.wandb_run_name += f\"-{int(time.time())}\"\n\n    # Randome seed\n    set_seed(args.seed)\n\n    # Init accelerator\n    accelerator = Accelerator(log_with=\"wandb\")\n\n    # Init wandb run\n    if accelerator.is_main_process:\n        wandb.init(project=args.wandb_project, name=args.wandb_run_name, config=vars(args))\n\n    # Display command arguments\n    accelerator.print(\"Arguments:\", args)\n\n    # General setup completed\n    accelerator.print(f\"Đang sử dụng {accelerator.num_processes} GPUs với DeepSpeed.\")\n\n    # Model & tokenizer\n    accelerator.print(\"Đang tải tokenizer và model...\")\n    tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n    model = AutoModelForCausalLM.from_pretrained(args.model_name)\n    \n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n        model.config.pad_token_id = model.config.eos_token_id\n\n    # Dataset\n    accelerator.print(\"Đang tải và xử lý dữ liệu...\")\n    raw_datasets = load_dataset(args.dataset_name, args.dataset_config)\n    raw_datasets['train'] = raw_datasets['train'].select(range(1000))\n    raw_datasets['validation'] = raw_datasets['validation'].select(range(100))\n    del raw_datasets['test']\n    \n    column_names = raw_datasets[\"train\"].column_names\n    text_column_name = \"text\" if \"text\" in column_names else column_names[0]\n\n    # Tokenize text\n    def tokenize_function(examples):\n        return tokenizer(examples[text_column_name])\n\n    tokenized_datasets = raw_datasets.map(\n        tokenize_function, batched=True, remove_columns=column_names, desc=\"Running tokenizer on dataset\"\n    )\n    accelerator.print(tokenized_datasets)\n\n    # Chunking\n    def group_texts(examples):\n        concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n        total_length = len(concatenated_examples[list(examples.keys())[0]])\n        total_length = (total_length // args.block_size) * args.block_size\n        result = {\n            k: [t[i : i + args.block_size] for i in range(0, total_length, args.block_size)]\n            for k, t in concatenated_examples.items()\n        }\n        result[\"labels\"] = result[\"input_ids\"].copy()\n        return result\n\n    lm_datasets = tokenized_datasets.map(\n        group_texts, batched=True, desc=f\"Grouping texts in chunks of {args.block_size}\"\n    )\n    accelerator.print(lm_datasets)\n    \n    train_dataset = lm_datasets[\"train\"]\n    eval_dataset = lm_datasets[\"validation\"]\n\n    # Data collator\n    data_collator = default_data_collator\n    train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=args.batch_size, collate_fn=data_collator)\n    eval_dataloader = DataLoader(eval_dataset, batch_size=args.batch_size, collate_fn=data_collator)\n\n    # Optimizer & scheduler\n    optimizer = DummyOptim(model.parameters())\n    scheduler = DummyScheduler(optimizer)\n\n    model, optimizer, train_dataloader, eval_dataloader, scheduler = accelerator.prepare(\n        model, optimizer, train_dataloader, eval_dataloader, scheduler\n    )\n\n    # Training\n    accelerator.print(\"\\n*** Bắt đầu huấn luyện ***\")\n    global_step = 0\n    start_training_time = time.time()\n    \n    for epoch in range(args.num_epochs):\n        model.train()\n        progress_bar = tqdm(\n            train_dataloader,\n            desc=f\"Epoch {epoch+1}/{args.num_epochs}\",\n            disable=not accelerator.is_local_main_process\n        )\n        start_epoch_time = time.time()\n        for batch in progress_bar:\n            outputs = model(**batch)\n            loss = outputs.loss\n            accelerator.backward(loss)\n            optimizer.step()\n            optimizer.zero_grad()\n            global_step += 1\n            \n            if global_step % args.logging_steps == 0:\n                learning_rate = scheduler.get_last_lr()[0]\n                log_metrics = {\n                    \"train_loss\": loss.item(),\n                    \"learning_rate\": learning_rate,\n                    \"epoch\": global_step / len(train_dataloader),\n                }\n                if accelerator.is_main_process: wandb.log(log_metrics, step=global_step)\n\n            if global_step % args.eval_steps == 0:\n                eval_loss, perplexity, eval_time = evaluate_model(model, eval_dataloader, accelerator, args)\n                log_metrics = {\n                    \"eval_loss\": eval_loss,\n                    \"perplexity\": perplexity,\n                    \"eval_time (s)\": eval_time,\n                }\n                if accelerator.is_main_process: wandb.log(log_metrics, step=global_step)\n                accelerator.print(f\"Step {global_step}: eval_loss = {eval_loss:.2f}\")\n\n            if accelerator.is_local_main_process:\n                progress_bar.set_postfix({\"loss\": loss.item(), \"step\": global_step})\n        \n        log_metrics = {\n            \"epoch_time (s)\": time.time() - start_epoch_time,\n            \"epoch\": epoch + 1,\n        }\n        if accelerator.is_main_process: wandb.log(log_metrics, step=global_step)\n\n    # Total training time\n    accelerator.wait_for_everyone()\n    total_training_time = time.time() - start_training_time\n    accelerator.print(f\"*** Huấn luyện hoàn tất trong: {total_training_time:.2f} giây ***\\n\")\n    \n    # Final evaluation\n    accelerator.print(\"*** Bắt đầu đánh giá cuối cùng ***\")\n    final_eval_loss, final_perplexity, _ = evaluate_model(model, eval_dataloader, accelerator, args)\n\n    if accelerator.is_main_process:\n        print(f\"*** Kết quả đánh giá cuối cùng trên tập validation ***\")\n        print(f\"Epoch: {args.num_epochs}\")\n        print(f\"Loss: {final_eval_loss:.4f}\")\n        print(f\"Perplexity: {final_perplexity:.4f}\")\n    \n    accelerator.end_training()\n    wandb.finish()\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!accelerate launch train_deepspeed_wandb.py \\\n    --model_name \"bigscience/bloom-560m\" \\\n    --batch_size 4 \\\n    --num_epochs 3 \\\n    --logging_steps 2 \\\n    --eval_steps 10 \\\n    --wandb_project \"PARADIS-bloom_560m\" \\\n    --wandb_run_name \"ZeRO_3\"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}