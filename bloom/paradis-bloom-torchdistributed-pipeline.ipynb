{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q transformers datasets deepspeed torch accelerate evaluate bitsandbytes pyyaml wandb","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-28T13:01:24.957778Z","iopub.execute_input":"2025-06-28T13:01:24.957986Z","iopub.status.idle":"2025-06-28T13:03:01.792878Z","shell.execute_reply.started":"2025-06-28T13:01:24.957968Z","shell.execute_reply":"2025-06-28T13:03:01.792031Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nWANDB_API_KEY = user_secrets.get_secret(\"WANDB_API_KEY\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T13:03:01.793717Z","iopub.execute_input":"2025-06-28T13:03:01.793917Z","iopub.status.idle":"2025-06-28T13:03:01.918063Z","shell.execute_reply.started":"2025-06-28T13:03:01.793896Z","shell.execute_reply":"2025-06-28T13:03:01.917518Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import wandb\nwandb.login(key=WANDB_API_KEY)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T13:03:01.919763Z","iopub.execute_input":"2025-06-28T13:03:01.919953Z","iopub.status.idle":"2025-06-28T13:03:10.481333Z","shell.execute_reply.started":"2025-06-28T13:03:01.919939Z","shell.execute_reply":"2025-06-28T13:03:10.480619Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile train_torch_pipeline.py\n\nimport torch\nimport torch.nn as nn\nimport time\nimport math\nimport argparse\nimport os\nimport wandb\nimport torch.distributed as dist\nfrom torch.distributed.pipelining import pipeline, PipelineStage, ScheduleGPipe\nfrom torch.optim import AdamW\nfrom torch.utils.data import DataLoader, DistributedSampler\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, default_data_collator, get_scheduler\nfrom transformers.models.bloom.modeling_bloom import build_alibi_tensor\nfrom datasets import load_dataset\nfrom tqdm.auto import tqdm\n\ndef set_seed(seed):\n    \"\"\"Hàm để set random seed cho reproducibility.\"\"\"\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\n# --- Các lớp cơ bản ---\nclass LanguageModelLoss(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.loss_fn = nn.CrossEntropyLoss()\n\n    def forward(self, logits, labels):\n        label_batch_size = labels.size(0)\n        logit_batch_size = logits.size(0)\n        if logit_batch_size > label_batch_size:\n            logits = logits[-label_batch_size:]\n        logits_view = logits.view(-1, logits.size(-1))\n        labels_view = labels.view(-1)\n        return self.loss_fn(logits_view, labels_view)\n\nclass BloomPipelineStage1(nn.Module):\n    def __init__(self, embedding, layernorm, blocks, num_attention_heads):\n        super().__init__()\n        self.embedding = embedding\n        self.layernorm = layernorm\n        self.blocks = nn.ModuleList(blocks)\n        self.num_attention_heads = num_attention_heads\n\n    def forward(self, input_ids, attention_mask):\n        hidden_states = self.embedding(input_ids)\n        hidden_states = self.layernorm(hidden_states)\n        alibi = build_alibi_tensor(attention_mask, self.num_attention_heads, dtype=hidden_states.dtype).to(hidden_states.device)\n        batch_size, seq_length = input_ids.shape\n        causal_mask = attention_mask[:, None, None, :].expand(batch_size, 1, seq_length, seq_length)\n        causal_mask = causal_mask.to(dtype=hidden_states.dtype) \n        causal_mask = causal_mask.contiguous()\n        for block in self.blocks:\n            outputs = block(hidden_states, alibi=alibi, attention_mask=causal_mask)\n            hidden_states = outputs[0]\n        return hidden_states, alibi, causal_mask\n\nclass BloomPipelineStage2(nn.Module):\n    def __init__(self, blocks, final_layernorm, lm_head):\n        super().__init__()\n        self.blocks = nn.ModuleList(blocks)\n        self.final_layernorm = final_layernorm\n        self.lm_head = lm_head\n\n    def forward(self, hidden_states, alibi, attention_mask):\n        for block in self.blocks:\n            outputs = block(hidden_states, alibi=alibi, attention_mask=attention_mask)\n            hidden_states = outputs[0]\n        hidden_states = self.final_layernorm(hidden_states)\n        logits = self.lm_head(hidden_states)\n        return logits\n\ndef main():\n    # --- Cấu hình tham số ---\n    parser = argparse.ArgumentParser(description=\"Finetune BLOOM with torch.distributed.pipelining\")\n    \n    parser.add_argument(\"--model_name\", type=str, default=\"bigscience/bloom-560m\")\n    parser.add_argument(\"--dataset_name\", type=str, default=\"Salesforce/wikitext\")\n    parser.add_argument(\"--dataset_config\", type=str, default=\"wikitext-2-raw-v1\")\n    \n    parser.add_argument(\"--seed\", type=int, default=42)\n    parser.add_argument(\"--num_epochs\", type=int, default=2)\n    parser.add_argument(\"--block_size\", type=int, default=128)\n    parser.add_argument(\"--batch_size\", type=int, default=2) \n    parser.add_argument(\"--lr\", type=float, default=3e-6)\n    parser.add_argument(\"--weight_decay\", type=float, default=0.01)\n    parser.add_argument(\"--warmup_steps\", type=int, default=20)\n    \n    parser.add_argument(\"--logging_steps\", type=int, default=5)\n    parser.add_argument(\"--wandb_project\", type=str, default=\"torch_pipeline_finetune\")\n    parser.add_argument(\"--wandb_run_name\", type=str, default=None)\n    \n    args = parser.parse_args()\n\n    # --- Khởi tạo môi trường phân tán ---\n    rank = int(os.environ[\"LOCAL_RANK\"])\n    world_size = int(os.environ[\"WORLD_SIZE\"])\n    device = torch.device(f\"cuda:{rank}\")\n    dist.init_process_group(backend=\"nccl\")\n    \n    set_seed(args.seed)\n\n    if args.wandb_run_name:\n        args.wandb_run_name += f\"-{int(time.time())}\"\n\n    if rank == 0:\n        wandb.init(project=args.wandb_project, name=args.wandb_run_name, config=vars(args))\n        print(\"Arguments:\", args)\n\n    # --- Tải Model và chia Stage ---\n    if rank == 0: print(\"Đang tải tokenizer và model...\")\n    model_hf = AutoModelForCausalLM.from_pretrained(args.model_name).cpu()\n    tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n    if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token\n\n    num_layers = len(model_hf.transformer.h)\n    layers_per_stage = num_layers // world_size\n    if rank == 0:\n        print(f\"Chia {num_layers} lớp thành {world_size} giai đoạn, mỗi giai đoạn {layers_per_stage} lớp.\")\n\n    if rank == 0:\n        stage_model = BloomPipelineStage1(\n            embedding=model_hf.transformer.word_embeddings,\n            layernorm=model_hf.transformer.word_embeddings_layernorm,\n            blocks=model_hf.transformer.h[:layers_per_stage],\n            num_attention_heads=model_hf.config.num_attention_heads\n        ).to(device)\n    else: # rank == 1\n        stage_model = BloomPipelineStage2(\n            blocks=model_hf.transformer.h[layers_per_stage:],\n            final_layernorm=model_hf.transformer.ln_f,\n            lm_head=model_hf.lm_head\n        ).to(device)\n    \n    del model_hf\n\n    # --- Tải và xử lý Dataset ---\n    if rank == 0: print(\"Đang tải và xử lý dữ liệu...\")\n    raw_datasets = load_dataset(args.dataset_name, args.dataset_config)\n    raw_datasets['train'] = raw_datasets['train'].select(range(1000))\n    splits_to_remove = [split for split in raw_datasets.keys() if split != 'train']\n    for split in splits_to_remove: del raw_datasets[split]\n\n    column_names = raw_datasets[\"train\"].column_names\n    text_column_name = \"text\" if \"text\" in column_names else column_names[0]\n\n    def tokenize_function(examples):\n        return tokenizer(examples[text_column_name])\n\n    tokenized_datasets = raw_datasets.map(\n        tokenize_function, batched=True, remove_columns=column_names, desc=\"Running tokenizer on dataset\" if rank == 0 else None\n    )\n\n    def group_texts(examples):\n        concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n        total_length = len(concatenated_examples[list(examples.keys())[0]])\n        total_length = (total_length // args.block_size) * args.block_size\n        result = {\n            k: [t[i : i + args.block_size] for i in range(0, total_length, args.block_size)]\n            for k, t in concatenated_examples.items()\n        }\n        result[\"labels\"] = result[\"input_ids\"].copy()\n        return result\n\n    lm_datasets = tokenized_datasets.map(\n        group_texts, batched=True, desc=f\"Grouping texts in chunks of {args.block_size}\" if rank == 0 else None\n    )\n    if rank == 0: print(lm_datasets)\n    \n    train_dataset = lm_datasets[\"train\"]\n    train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank, shuffle=True)\n    dataloader = DataLoader(train_dataset, batch_size=args.batch_size, collate_fn=default_data_collator, sampler=train_sampler)\n\n    # --- Thiết lập Pipeline ---\n    stage = PipelineStage(stage_model, rank, world_size, device)\n    loss_fn = LanguageModelLoss().to(device)\n    n_microbatches = world_size \n    schedule = ScheduleGPipe(stage, n_microbatches, loss_fn=loss_fn)\n    \n    # --- Optimizer & Scheduler ---\n    optimizer = AdamW(stage_model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n    num_training_steps = args.num_epochs * len(dataloader)\n    lr_scheduler = get_scheduler(\n        name=\"linear\",\n        optimizer=optimizer,\n        num_warmup_steps=args.warmup_steps,\n        num_training_steps=num_training_steps,\n    )\n\n    # --- Vòng lặp huấn luyện ---\n    if rank == 0: print(\"\\n*** Bắt đầu huấn luyện ***\")\n    \n    global_step = 0\n    start_training_time = time.time()\n    \n    for epoch in range(args.num_epochs):\n        stage_model.train()\n        train_sampler.set_epoch(epoch)\n        start_epoch_time = time.time()\n        if rank == 0:\n            print(f\"\\n--- Bắt đầu Epoch {epoch+1}/{args.num_epochs} ---\")\n        \n        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}\", disable=(rank != 0))\n\n        for batch in progress_bar:\n            optimizer.zero_grad()\n            \n            inputs = (batch['input_ids'].to(device), batch['attention_mask'].to(device))\n            targets = batch['labels'].to(device)\n            \n            # Khởi tạo tensor loss trên tất cả các rank\n            loss = torch.tensor(0.0, device=device)\n            \n            if rank == 0:\n                schedule.step(*inputs)\n            elif rank == world_size - 1: # Stage cuối cùng\n                losses = []\n                schedule.step(target=targets, losses=losses)\n                loss = losses[0]\n            else: # Các stage ở giữa\n                schedule.step()\n\n            optimizer.step()\n            lr_scheduler.step()\n            \n            global_step += 1\n            \n            # --- SỬA LỖI: Gửi loss từ rank cuối về rank 0 để log ---\n            # 1. Phát sóng (broadcast) giá trị loss từ rank cuối đến tất cả các rank khác.\n            dist.broadcast(loss, src=world_size - 1)\n            \n            # 2. Bây giờ tất cả các rank đều có giá trị loss, nhưng chỉ rank 0 mới thực hiện logging.\n            if rank == 0:\n                progress_bar.set_postfix({\"loss\": loss.item()})\n                if global_step % args.logging_steps == 0:\n                    log_metrics = {\n                        \"train_loss\": loss.item(),\n                        \"learning_rate\": lr_scheduler.get_last_lr()[0],\n                        \"epoch\": epoch + (progress_bar.n / progress_bar.total),\n                    }\n                    wandb.log(log_metrics, step=global_step)\n            # --- KẾT THÚC SỬA LỖI ---\n\n        dist.barrier()\n        if rank == 0:\n            epoch_time = time.time() - start_epoch_time\n            print(f\"--- Epoch {epoch+1} hoàn tất trong {epoch_time:.2f} giây ---\")\n            wandb.log({\n                \"epoch_time (s)\": epoch_time,\n                \"epoch\": epoch + 1\n            }, step=global_step)\n\n    # --- Kết thúc ---\n    dist.barrier()\n    if rank == 0:\n        total_training_time = time.time() - start_training_time\n        print(f\"\\n*** Huấn luyện hoàn tất trong: {total_training_time:.2f} giây ***\")\n        wandb.finish()\n        \n    dist.destroy_process_group()\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T13:27:12.577259Z","iopub.execute_input":"2025-06-28T13:27:12.577654Z","iopub.status.idle":"2025-06-28T13:27:12.588989Z","shell.execute_reply.started":"2025-06-28T13:27:12.577624Z","shell.execute_reply":"2025-06-28T13:27:12.588196Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!torchrun --nnodes 1 --nproc_per_node 2 /kaggle/working/train_torch_pipeline.py \\\n    --model_name \"bigscience/bloom-1b7\" \\\n    --batch_size 4 \\\n    --num_epochs 3 \\\n    --logging_steps 2 \\\n    --wandb_project \"PARADIS-bloom_1b7\" \\\n    --wandb_run_name \"TD_Pipeline\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T13:27:13.545029Z","iopub.execute_input":"2025-06-28T13:27:13.545299Z","iopub.status.idle":"2025-06-28T13:29:38.127123Z","shell.execute_reply.started":"2025-06-28T13:27:13.545280Z","shell.execute_reply":"2025-06-28T13:29:38.126174Z"}},"outputs":[],"execution_count":null}]}