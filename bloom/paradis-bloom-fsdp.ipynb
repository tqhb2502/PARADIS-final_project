{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q transformers datasets deepspeed torch accelerate evaluate bitsandbytes pyyaml wandb","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-28T11:12:31.835140Z","iopub.execute_input":"2025-06-28T11:12:31.835353Z","iopub.status.idle":"2025-06-28T11:14:07.690025Z","shell.execute_reply.started":"2025-06-28T11:12:31.835336Z","shell.execute_reply":"2025-06-28T11:14:07.689336Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nWANDB_API_KEY = user_secrets.get_secret(\"WANDB_API_KEY\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T11:14:07.692083Z","iopub.execute_input":"2025-06-28T11:14:07.692340Z","iopub.status.idle":"2025-06-28T11:14:07.809425Z","shell.execute_reply.started":"2025-06-28T11:14:07.692310Z","shell.execute_reply":"2025-06-28T11:14:07.808900Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import wandb\nwandb.login(key=WANDB_API_KEY)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T11:14:07.809975Z","iopub.execute_input":"2025-06-28T11:14:07.810152Z","iopub.status.idle":"2025-06-28T11:14:16.156954Z","shell.execute_reply.started":"2025-06-28T11:14:07.810137Z","shell.execute_reply":"2025-06-28T11:14:16.156422Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport yaml\n\n# --- Tạo thư mục cấu hình cho Accelerate ---\naccelerate_config_dir = os.path.expanduser(\"~/.cache/huggingface/accelerate\")\nos.makedirs(accelerate_config_dir, exist_ok=True)\n\n# --- Định nghĩa cấu hình Accelerate cho FSDP ---\n# Đây là phần quan trọng nhất.\nfsdp_config = {\n    'compute_environment': 'LOCAL_MACHINE',\n    'distributed_type': 'FSDP',  # THAY ĐỔI QUAN TRỌNG: Từ DEEPSPEED sang FSDP\n    'downcast_bf16': 'no',\n    'fsdp_config': {\n        'fsdp_auto_wrap_policy': 'TRANSFORMER_BASED_WRAP',\n        'fsdp_backward_prefetch': 'BACKWARD_PRE',\n        'fsdp_cpu_ram_efficient_loading': True,\n        'fsdp_offload_params': True,  # Tương đương offload_param của ZeRO-3 -> Đẩy tham số model sang CPU\n        'fsdp_sharding_strategy': 1,  # 1 = FULL_SHARD (tương đương ZeRO-3), 2 = SHARD_GRAD_OP (tương đương ZeRO-2)\n        'fsdp_state_dict_type': 'FULL_STATE_DICT',\n        'fsdp_sync_module_states': True,\n        # Quan trọng: Phải cho FSDP biết khối layer cơ bản của model để nó bọc lại. Với BLOOM, đó là 'BloomBlock'.\n        'fsdp_transformer_layer_cls_to_wrap': 'BloomBlock', \n        'fsdp_use_orig_params': True,\n    },\n    'machine_rank': 0,\n    'main_process_ip': None,\n    'main_process_port': None,\n    'main_training_function': 'main',\n    'mixed_precision': 'bf16', # bf16 thường ổn định và tốt hơn fp16 cho FSDP trên các GPU mới\n    'num_machines': 1,\n    'num_processes': 2,  # Vẫn sử dụng 2 GPU\n    'use_cpu': False,\n}\n\n# --- Ghi file cấu hình Accelerate ---\nconfig_path = os.path.join(accelerate_config_dir, \"default_config.yaml\")\nwith open(config_path, 'w') as f:\n    yaml.dump(fsdp_config, f)\n\nprint(f\"File cấu hình Accelerate cho FSDP đã được tạo tại: {config_path}\")\nprint(\"\\nBây giờ không cần file ds_zero3_config.json nữa.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T11:14:16.157598Z","iopub.execute_input":"2025-06-28T11:14:16.157976Z","iopub.status.idle":"2025-06-28T11:14:16.167093Z","shell.execute_reply.started":"2025-06-28T11:14:16.157944Z","shell.execute_reply":"2025-06-28T11:14:16.166565Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile train_fsdp.py\n\nimport torch\nimport time\nimport math\nimport argparse\nimport wandb\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, default_data_collator, get_scheduler\nfrom torch.optim import AdamW\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader\nfrom accelerate import Accelerator\nfrom tqdm.auto import tqdm\n\ndef set_seed(seed):\n    \"\"\"Hàm để set random seed cho reproducibility.\"\"\"\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\ndef evaluate_model(model, dataloader, accelerator, args):\n    \"\"\"Hàm để đánh giá model và trả về loss, perplexity.\"\"\"\n    model.eval()\n    losses = []\n    eval_start_time = time.time()\n    for batch in dataloader:\n        with torch.no_grad():\n            outputs = model(**batch)\n        loss = outputs.loss\n        losses.append(accelerator.gather_for_metrics(loss.repeat(args.batch_size)))\n\n    losses = torch.cat(losses)\n    if accelerator.num_processes > 1:\n        losses = losses[:len(dataloader.dataset)]\n    try:\n        eval_loss = torch.mean(losses)\n        perplexity = math.exp(eval_loss)\n    except OverflowError:\n        eval_loss = torch.tensor(float(\"inf\"))\n        perplexity = float(\"inf\")\n    \n    eval_time = time.time() - eval_start_time\n    model.train()\n    return eval_loss.item(), perplexity, eval_time\n\ndef main():\n    # --- Cấu hình các tham số ---\n    parser = argparse.ArgumentParser(description=\"Finetune BLOOM with PyTorch FSDP and W&B\")\n    \n    # Tham số Model & Dataset\n    parser.add_argument(\"--model_name\", type=str, default=\"bigscience/bloom-560m\")\n    parser.add_argument(\"--dataset_name\", type=str, default=\"Salesforce/wikitext\")\n    parser.add_argument(\"--dataset_config\", type=str, default=\"wikitext-2-raw-v1\")\n    \n    # Tham số Huấn luyện\n    parser.add_argument(\"--seed\", type=int, default=42)\n    parser.add_argument(\"--num_epochs\", type=int, default=2)\n    parser.add_argument(\"--batch_size\", type=int, default=4)\n    parser.add_argument(\"--block_size\", type=int, default=128)\n    parser.add_argument(\"--lr\", type=float, default=3e-6, help=\"Learning rate\")\n    parser.add_argument(\"--weight_decay\", type=float, default=0.01)\n    parser.add_argument(\"--warmup_steps\", type=int, default=20)\n\n    # Tham số Logging\n    parser.add_argument(\"--logging_steps\", type=int, default=5)\n    parser.add_argument(\"--eval_steps\", type=int, default=20)\n    parser.add_argument(\"--wandb_project\", type=str, default=\"fsdp_bloom_finetune\")\n    parser.add_argument(\"--wandb_run_name\", type=str, default=None)\n    \n    args = parser.parse_args()\n    \n    if args.wandb_run_name:\n        args.wandb_run_name += f\"-{int(time.time())}\"\n\n    # --- Thiết lập chung ---\n    set_seed(args.seed)\n    accelerator = Accelerator(log_with=\"wandb\")\n\n    if accelerator.is_main_process:\n        wandb.init(project=args.wandb_project, name=args.wandb_run_name, config=vars(args))\n\n    accelerator.print(\"Arguments:\", args)\n    accelerator.print(f\"Đang sử dụng {accelerator.num_processes} GPUs với PyTorch FSDP.\")\n\n    # --- Tải Model & Tokenizer ---\n    accelerator.print(\"Đang tải tokenizer và model...\")\n    tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n    model = AutoModelForCausalLM.from_pretrained(args.model_name)\n    \n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n        model.config.pad_token_id = model.config.eos_token_id\n\n    # --- Tải và xử lý Dataset ---\n    accelerator.print(\"Đang tải và xử lý dữ liệu...\")\n    raw_datasets = load_dataset(args.dataset_name, args.dataset_config)\n    raw_datasets['train'] = raw_datasets['train'].select(range(1000))\n    raw_datasets['validation'] = raw_datasets['validation'].select(range(100))\n    del raw_datasets['test']\n    \n    column_names = raw_datasets[\"train\"].column_names\n    text_column_name = \"text\" if \"text\" in column_names else column_names[0]\n\n    def tokenize_function(examples):\n        return tokenizer(examples[text_column_name])\n\n    tokenized_datasets = raw_datasets.map(\n        tokenize_function, batched=True, remove_columns=column_names, desc=\"Running tokenizer on dataset\"\n    )\n    accelerator.print(tokenized_datasets)\n\n    def group_texts(examples):\n        concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n        total_length = len(concatenated_examples[list(examples.keys())[0]])\n        total_length = (total_length // args.block_size) * args.block_size\n        result = {\n            k: [t[i : i + args.block_size] for i in range(0, total_length, args.block_size)]\n            for k, t in concatenated_examples.items()\n        }\n        result[\"labels\"] = result[\"input_ids\"].copy()\n        return result\n\n    lm_datasets = tokenized_datasets.map(\n        group_texts, batched=True, desc=f\"Grouping texts in chunks of {args.block_size}\"\n    )\n    accelerator.print(lm_datasets)\n    \n    train_dataset = lm_datasets[\"train\"]\n    eval_dataset = lm_datasets[\"validation\"]\n\n    data_collator = default_data_collator\n    train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=args.batch_size, collate_fn=data_collator)\n    eval_dataloader = DataLoader(eval_dataset, batch_size=args.batch_size, collate_fn=data_collator)\n\n    # --- Optimizer & Scheduler ---\n    optimizer = AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n    \n    num_training_steps = args.num_epochs * len(train_dataloader)\n    lr_scheduler = get_scheduler(\n        name=\"linear\",\n        optimizer=optimizer,\n        num_warmup_steps=args.warmup_steps,\n        num_training_steps=num_training_steps,\n    )\n\n    # --- Chuẩn bị với Accelerator ---\n    model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(\n        model, optimizer, train_dataloader, eval_dataloader, lr_scheduler\n    )\n\n    # --- Vòng lặp huấn luyện ---\n    accelerator.print(\"\\n*** Bắt đầu huấn luyện ***\")\n    global_step = 0\n    start_training_time = time.time()\n    \n    for epoch in range(args.num_epochs):\n        model.train()\n        progress_bar = tqdm(\n            train_dataloader,\n            desc=f\"Epoch {epoch+1}/{args.num_epochs}\",\n            disable=not accelerator.is_local_main_process\n        )\n        start_epoch_time = time.time()\n        for batch in progress_bar:\n            outputs = model(**batch)\n            loss = outputs.loss\n            accelerator.backward(loss)\n            optimizer.step()\n            lr_scheduler.step()\n            optimizer.zero_grad()\n            global_step += 1\n            \n            if global_step % args.logging_steps == 0:\n                log_metrics = {\n                    \"train_loss\": loss.item(),\n                    \"learning_rate\": lr_scheduler.get_last_lr()[0],\n                    \"epoch\": global_step / len(train_dataloader),\n                }\n                if accelerator.is_main_process: wandb.log(log_metrics, step=global_step)\n\n            if global_step % args.eval_steps == 0:\n                eval_loss, perplexity, eval_time = evaluate_model(model, eval_dataloader, accelerator, args)\n                log_metrics = {\n                    \"eval_loss\": eval_loss,\n                    \"perplexity\": perplexity,\n                    \"eval_time (s)\": eval_time,\n                }\n                if accelerator.is_main_process: wandb.log(log_metrics, step=global_step)\n                accelerator.print(f\"Step {global_step}: eval_loss = {eval_loss:.2f}\")\n\n            if accelerator.is_main_process:\n                progress_bar.set_postfix({\"loss\": loss.item(), \"step\": global_step})\n        \n        log_metrics = {\n            \"epoch_time (s)\": time.time() - start_epoch_time,\n            \"epoch\": epoch + 1,\n        }\n        if accelerator.is_main_process: wandb.log(log_metrics, step=global_step)\n\n    # Total training time\n    accelerator.wait_for_everyone()\n    total_training_time = time.time() - start_training_time\n    accelerator.print(f\"*** Huấn luyện hoàn tất trong: {total_training_time:.2f} giây ***\\n\")\n    \n    # --- Kết thúc huấn luyện ---\n    accelerator.print(\"*** Bắt đầu đánh giá cuối cùng ***\")\n    final_eval_loss, final_perplexity, _ = evaluate_model(model, eval_dataloader, accelerator, args)\n\n    if accelerator.is_main_process:\n        print(f\"*** Kết quả đánh giá cuối cùng trên tập validation ***\")\n        print(f\"Epoch: {args.num_epochs}\")\n        print(f\"Loss: {final_eval_loss:.4f}\")\n        print(f\"Perplexity: {final_perplexity:.4f}\")\n    \n    accelerator.end_training()\n    wandb.finish()\n    \nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T11:14:16.167764Z","iopub.execute_input":"2025-06-28T11:14:16.167930Z","iopub.status.idle":"2025-06-28T11:14:16.182809Z","shell.execute_reply.started":"2025-06-28T11:14:16.167917Z","shell.execute_reply":"2025-06-28T11:14:16.182130Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!accelerate launch --multi_gpu /kaggle/working/train_fsdp.py \\\n    --model_name \"bigscience/bloom-560m\" \\\n    --batch_size 4 \\\n    --num_epochs 3 \\\n    --logging_steps 2 \\\n    --eval_steps 10 \\\n    --wandb_project \"PARADIS-bloom_560m\" \\\n    --wandb_run_name \"FSDP\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T11:14:16.183397Z","iopub.execute_input":"2025-06-28T11:14:16.183560Z","iopub.status.idle":"2025-06-28T11:20:04.807241Z","shell.execute_reply.started":"2025-06-28T11:14:16.183547Z","shell.execute_reply":"2025-06-28T11:20:04.806527Z"}},"outputs":[],"execution_count":null}]}