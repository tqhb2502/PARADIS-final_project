{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nWANDB_API_KEY = user_secrets.get_secret(\"WANDB_API_KEY\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T13:39:11.301960Z","iopub.execute_input":"2025-06-28T13:39:11.302272Z","iopub.status.idle":"2025-06-28T13:39:11.408499Z","shell.execute_reply.started":"2025-06-28T13:39:11.302228Z","shell.execute_reply":"2025-06-28T13:39:11.408000Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import wandb\nwandb.login(key=WANDB_API_KEY)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T13:39:11.409420Z","iopub.execute_input":"2025-06-28T13:39:11.409620Z","iopub.status.idle":"2025-06-28T13:39:18.326243Z","shell.execute_reply.started":"2025-06-28T13:39:11.409604Z","shell.execute_reply":"2025-06-28T13:39:18.325638Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile train_single_gpu.py\n\nimport torch\nimport time\nimport math\nimport argparse\nimport wandb\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, default_data_collator, get_scheduler\nfrom torch.optim import AdamW\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader\nfrom tqdm.auto import tqdm\n\ndef set_seed(seed):\n    \"\"\"Hàm để set random seed cho reproducibility.\"\"\"\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\ndef evaluate_model(model, dataloader, device):\n    \"\"\"Hàm đánh giá model trên một GPU duy nhất.\"\"\"\n    model.eval()\n    losses = []\n    eval_start_time = time.time()\n    for batch in dataloader:\n        # Chuyển batch dữ liệu sang device\n        batch = {k: v.to(device) for k, v in batch.items()}\n        with torch.no_grad():\n            outputs = model(**batch)\n        \n        loss = outputs.loss\n        losses.append(loss.item())\n\n    try:\n        eval_loss = sum(losses) / len(losses)\n        perplexity = math.exp(eval_loss)\n    except OverflowError:\n        eval_loss = float(\"inf\")\n        perplexity = float(\"inf\")\n    \n    eval_time = time.time() - eval_start_time\n    model.train() # Chuyển model về lại chế độ train\n    return eval_loss, perplexity, eval_time\n\ndef main():\n    # --- Cấu hình tham số ---\n    parser = argparse.ArgumentParser(description=\"Finetune BLOOM on a single GPU\")\n    \n    parser.add_argument(\"--model_name\", type=str, default=\"bigscience/bloom-560m\")\n    parser.add_argument(\"--dataset_name\", type=str, default=\"Salesforce/wikitext\")\n    parser.add_argument(\"--dataset_config\", type=str, default=\"wikitext-2-raw-v1\")\n    \n    parser.add_argument(\"--seed\", type=int, default=42)\n    parser.add_argument(\"--num_epochs\", type=int, default=2)\n    parser.add_argument(\"--batch_size\", type=int, default=4)\n    parser.add_argument(\"--block_size\", type=int, default=128)\n    \n    # Thêm các tham số cho optimizer và scheduler\n    parser.add_argument(\"--lr\", type=float, default=3e-6)\n    parser.add_argument(\"--weight_decay\", type=float, default=0.01)\n    parser.add_argument(\"--warmup_steps\", type=int, default=20)\n\n    parser.add_argument(\"--logging_steps\", type=int, default=5)\n    parser.add_argument(\"--eval_steps\", type=int, default=20)\n    parser.add_argument(\"--wandb_project\", type=str, default=\"single_gpu_finetune\")\n    parser.add_argument(\"--wandb_run_name\", type=str, default=None)\n    \n    args = parser.parse_args()\n    \n    if args.wandb_run_name:\n        args.wandb_run_name += f\"-{int(time.time())}\"\n\n    # --- Thiết lập chung ---\n    set_seed(args.seed)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Đang sử dụng device: {device}\")\n\n    # Khởi tạo W&B\n    wandb.init(project=args.wandb_project, name=args.wandb_run_name, config=vars(args))\n\n    # --- Tải Model & Tokenizer ---\n    print(\"Đang tải tokenizer và model...\")\n    tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n    model = AutoModelForCausalLM.from_pretrained(args.model_name)\n    model.to(device) # Chuyển model sang device\n    \n    if tokenizer.pad_token is None:\n        tokenizer.pad_token = tokenizer.eos_token\n        model.config.pad_token_id = model.config.eos_token_id\n\n    # --- Tải và xử lý Dataset ---\n    print(\"Đang tải và xử lý dữ liệu...\")\n    raw_datasets = load_dataset(args.dataset_name, args.dataset_config)\n    raw_datasets['train'] = raw_datasets['train'].select(range(1000))\n    raw_datasets['validation'] = raw_datasets['validation'].select(range(100))\n    del raw_datasets['test']\n    \n    column_names = raw_datasets[\"train\"].column_names\n    text_column_name = \"text\" if \"text\" in column_names else column_names[0]\n\n    def tokenize_function(examples):\n        return tokenizer(examples[text_column_name])\n\n    tokenized_datasets = raw_datasets.map(\n        tokenize_function, batched=True, remove_columns=column_names, desc=\"Running tokenizer on dataset\"\n    )\n\n    def group_texts(examples):\n        concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n        total_length = len(concatenated_examples[list(examples.keys())[0]])\n        total_length = (total_length // args.block_size) * args.block_size\n        result = {\n            k: [t[i : i + args.block_size] for i in range(0, total_length, args.block_size)]\n            for k, t in concatenated_examples.items()\n        }\n        result[\"labels\"] = result[\"input_ids\"].copy()\n        return result\n\n    lm_datasets = tokenized_datasets.map(\n        group_texts, batched=True, desc=f\"Grouping texts in chunks of {args.block_size}\"\n    )\n    print(lm_datasets)\n    \n    train_dataset = lm_datasets[\"train\"]\n    eval_dataset = lm_datasets[\"validation\"]\n\n    data_collator = default_data_collator\n    train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=args.batch_size, collate_fn=data_collator)\n    eval_dataloader = DataLoader(eval_dataset, batch_size=args.batch_size, collate_fn=data_collator)\n\n    # --- Optimizer & Scheduler ---\n    optimizer = AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n    num_training_steps = args.num_epochs * len(train_dataloader)\n    lr_scheduler = get_scheduler(\n        name=\"linear\",\n        optimizer=optimizer,\n        num_warmup_steps=args.warmup_steps,\n        num_training_steps=num_training_steps,\n    )\n\n    # --- Vòng lặp huấn luyện ---\n    print(\"\\n*** Bắt đầu huấn luyện ***\")\n    global_step = 0\n    start_training_time = time.time()\n    \n    for epoch in range(args.num_epochs):\n        model.train()\n        progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{args.num_epochs}\")\n        start_epoch_time = time.time()\n        \n        for batch in progress_bar:\n            # Chuyển batch dữ liệu sang device\n            batch = {k: v.to(device) for k, v in batch.items()}\n            \n            outputs = model(**batch)\n            loss = outputs.loss\n            \n            loss.backward() # PyTorch backpropagation\n            \n            optimizer.step()\n            lr_scheduler.step()\n            optimizer.zero_grad()\n            \n            global_step += 1\n            \n            # Logging\n            if global_step % args.logging_steps == 0:\n                log_metrics = {\n                    \"train_loss\": loss.item(),\n                    \"learning_rate\": lr_scheduler.get_last_lr()[0],\n                    \"epoch\": global_step / len(train_dataloader),\n                }\n                wandb.log(log_metrics, step=global_step)\n\n            # Đánh giá\n            if global_step % args.eval_steps == 0:\n                eval_loss, perplexity, eval_time = evaluate_model(model, eval_dataloader, device)\n                log_metrics = {\n                    \"eval_loss\": eval_loss,\n                    \"perplexity\": perplexity,\n                    \"eval_time (s)\": eval_time,\n                }\n                wandb.log(log_metrics, step=global_step)\n                print(f\"\\nStep {global_step}: eval_loss = {eval_loss:.2f}\")\n\n            progress_bar.set_postfix({\"loss\": loss.item()})\n        \n        # Log thời gian mỗi epoch\n        epoch_time = time.time() - start_epoch_time\n        log_metrics = {\n            \"epoch_time (s)\": epoch_time,\n            \"epoch\": epoch + 1,\n        }\n        wandb.log(log_metrics, step=global_step)\n        print(f\"--- Epoch {epoch+1} hoàn tất trong {epoch_time:.2f} giây ---\")\n\n    # --- Kết thúc ---\n    total_training_time = time.time() - start_training_time\n    print(f\"*** Huấn luyện hoàn tất trong: {total_training_time:.2f} giây ***\\n\")\n    \n    print(\"*** Bắt đầu đánh giá cuối cùng ***\")\n    final_eval_loss, final_perplexity, _ = evaluate_model(model, eval_dataloader, device)\n\n    print(f\"*** Kết quả đánh giá cuối cùng trên tập validation ***\")\n    print(f\"Epoch: {args.num_epochs}\")\n    print(f\"Loss: {final_eval_loss:.4f}\")\n    print(f\"Perplexity: {final_perplexity:.4f}\")\n    \n    wandb.finish()\n\nif __name__ == \"__main__\":\n    main()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-28T13:39:18.326929Z","iopub.execute_input":"2025-06-28T13:39:18.327244Z","iopub.status.idle":"2025-06-28T13:39:18.335229Z","shell.execute_reply.started":"2025-06-28T13:39:18.327226Z","shell.execute_reply":"2025-06-28T13:39:18.334505Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python train_single_gpu.py \\\n    --model_name \"bigscience/bloom-1b7\" \\\n    --batch_size 4 \\\n    --num_epochs 3 \\\n    --logging_steps 2 \\\n    --eval_steps 10 \\\n    --wandb_project \"PARADIS-bloom_1b7\" \\\n    --wandb_run_name \"1GPU\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T13:52:12.965689Z","iopub.execute_input":"2025-06-28T13:52:12.966461Z","iopub.status.idle":"2025-06-28T13:52:35.683520Z","shell.execute_reply.started":"2025-06-28T13:52:12.966431Z","shell.execute_reply":"2025-06-28T13:52:35.682581Z"}},"outputs":[],"execution_count":null}]}